using System.Collections.Generic;
using System.Text.Json.Serialization;

namespace ConduitLLM.Providers.InternalModels;

/// <summary>
/// Provides data models for interacting with the Ollama API.
/// </summary>
/// <remarks>
/// These models are based on the Ollama API documentation:
/// https://github.com/ollama/ollama/blob/main/docs/api.md (as of early 2024)
/// </remarks>

#region Chat Completions

/// <summary>
/// Represents a message in an Ollama conversation.
/// </summary>
public record OllamaMessage
{
    /// <summary>
    /// Gets the role of the message sender (e.g., "user", "assistant").
    /// </summary>
    [JsonPropertyName("role")]
    public string Role { get; init; } = string.Empty;

    /// <summary>
    /// Gets the text content of the message.
    /// </summary>
    [JsonPropertyName("content")]
    public string Content { get; init; } = string.Empty;

    /// <summary>
    /// Gets an optional list of base64-encoded images for multimodal models.
    /// </summary>
    [JsonPropertyName("images")]
    [JsonIgnore(Condition = JsonIgnoreCondition.WhenWritingNull)]
    public List<string>? Images { get; init; }
}

/// <summary>
/// Represents a request to the Ollama chat API endpoint.
/// </summary>
public record OllamaChatRequest
{
    /// <summary>
    /// Gets the name of the model to use for generation.
    /// </summary>
    [JsonPropertyName("model")]
    public string Model { get; init; } = string.Empty;

    /// <summary>
    /// Gets the list of messages in the conversation history.
    /// </summary>
    [JsonPropertyName("messages")]
    public List<OllamaMessage> Messages { get; init; } = new();

    /// <summary>
    /// Gets the optional output format specification (e.g., "json").
    /// </summary>
    [JsonPropertyName("format")]
    [JsonIgnore(Condition = JsonIgnoreCondition.WhenWritingNull)]
    public string? Format { get; init; }

    /// <summary>
    /// Gets the optional generation parameters to customize model behavior.
    /// </summary>
    [JsonPropertyName("options")]
    [JsonIgnore(Condition = JsonIgnoreCondition.WhenWritingNull)]
    public OllamaOptions? Options { get; init; }

    /// <summary>
    /// Gets a value indicating whether to stream the response.
    /// </summary>
    [JsonPropertyName("stream")]
    public bool Stream { get; init; } = false;

    /// <summary>
    /// Gets the optional model keep-alive duration (e.g., "5m").
    /// </summary>
    /// <remarks>
    /// This parameter tells Ollama how long to keep the model loaded in memory after the request.
    /// </remarks>
    [JsonPropertyName("keep_alive")]
    [JsonIgnore(Condition = JsonIgnoreCondition.WhenWritingNull)]
    public string? KeepAlive { get; init; }
}

/// <summary>
/// Represents a response from the Ollama chat API endpoint.
/// </summary>
public record OllamaChatResponse
{
    /// <summary>
    /// Gets the name of the model that generated the response.
    /// </summary>
    [JsonPropertyName("model")]
    public string Model { get; init; } = string.Empty;

    /// <summary>
    /// Gets the ISO 8601 timestamp when the response was created.
    /// </summary>
    [JsonPropertyName("created_at")]
    public string CreatedAt { get; init; } = string.Empty;

    /// <summary>
    /// Gets the message generated by the model.
    /// </summary>
    [JsonPropertyName("message")]
    public OllamaMessage? Message { get; init; }

    /// <summary>
    /// Gets a value indicating whether the generation is complete.
    /// </summary>
    [JsonPropertyName("done")]
    public bool Done { get; init; }

    /// <summary>
    /// Gets the total duration of the generation process in nanoseconds.
    /// </summary>
    /// <remarks>
    /// This value is only present when Done is true.
    /// </remarks>
    [JsonPropertyName("total_duration")]
    public long? TotalDuration { get; init; }

    /// <summary>
    /// Gets the time taken to load the model in nanoseconds.
    /// </summary>
    /// <remarks>
    /// This value is only present when Done is true.
    /// </remarks>
    [JsonPropertyName("load_duration")]
    public long? LoadDuration { get; init; }

    /// <summary>
    /// Gets the number of tokens in the prompt.
    /// </summary>
    /// <remarks>
    /// This value is only present when Done is true.
    /// </remarks>
    [JsonPropertyName("prompt_eval_count")]
    public int? PromptEvalCount { get; init; }

    /// <summary>
    /// Gets the time taken to process the prompt in nanoseconds.
    /// </summary>
    /// <remarks>
    /// This value is only present when Done is true.
    /// </remarks>
    [JsonPropertyName("prompt_eval_duration")]
    public long? PromptEvalDuration { get; init; }

    /// <summary>
    /// Gets the number of tokens in the completion.
    /// </summary>
    /// <remarks>
    /// This value is only present when Done is true.
    /// </remarks>
    [JsonPropertyName("eval_count")]
    public int? EvalCount { get; init; }

    /// <summary>
    /// Gets the time taken to generate the completion in nanoseconds.
    /// </summary>
    /// <remarks>
    /// This value is only present when Done is true.
    /// </remarks>
    [JsonPropertyName("eval_duration")]
    public long? EvalDuration { get; init; }
}

/// <summary>
/// Represents a chunk in a streaming response from the Ollama chat API.
/// </summary>
/// <remarks>
/// Inherits properties from OllamaChatResponse.
/// In streaming mode, the Message property will contain the delta content for each chunk.
/// The Done property indicates the final chunk, which includes usage statistics.
/// </remarks>
public record OllamaStreamChunk : OllamaChatResponse
{
}

#endregion

#region Model Listing

/// <summary>
/// Represents a response from the Ollama models/tags API endpoint.
/// </summary>
public record OllamaTagsResponse
{
    /// <summary>
    /// Gets the list of available models.
    /// </summary>
    [JsonPropertyName("models")]
    public List<OllamaModelInfo> Models { get; init; } = new();
}

/// <summary>
/// Represents information about an Ollama model.
/// </summary>
public record OllamaModelInfo
{
    /// <summary>
    /// Gets the name of the model (e.g., "llama3:latest").
    /// </summary>
    [JsonPropertyName("name")]
    public string Name { get; init; } = string.Empty;

    /// <summary>
    /// Gets the ISO 8601 timestamp when the model was last modified.
    /// </summary>
    [JsonPropertyName("modified_at")]
    public string ModifiedAt { get; init; } = string.Empty;

    /// <summary>
    /// Gets the size of the model in bytes.
    /// </summary>
    [JsonPropertyName("size")]
    public long Size { get; init; }

    /// <summary>
    /// Gets the unique digest identifier of the model.
    /// </summary>
    [JsonPropertyName("digest")]
    public string Digest { get; init; } = string.Empty;

    /// <summary>
    /// Gets the detailed information about the model.
    /// </summary>
    [JsonPropertyName("details")]
    public OllamaModelDetails? Details { get; init; }
}

/// <summary>
/// Represents detailed information about an Ollama model.
/// </summary>
public record OllamaModelDetails
{
    /// <summary>
    /// Gets the format of the model file.
    /// </summary>
    [JsonPropertyName("format")]
    public string Format { get; init; } = string.Empty;

    /// <summary>
    /// Gets the primary family/architecture of the model.
    /// </summary>
    [JsonPropertyName("family")]
    public string Family { get; init; } = string.Empty;

    /// <summary>
    /// Gets the list of model families/architectures this model belongs to.
    /// </summary>
    [JsonPropertyName("families")]
    public List<string>? Families { get; init; }

    /// <summary>
    /// Gets the parameter size of the model (e.g., "7B").
    /// </summary>
    [JsonPropertyName("parameter_size")]
    public string ParameterSize { get; init; } = string.Empty;

    /// <summary>
    /// Gets the quantization level of the model (e.g., "Q4_0").
    /// </summary>
    [JsonPropertyName("quantization_level")]
    public string QuantizationLevel { get; init; } = string.Empty;
}

#endregion

#region Embeddings

/// <summary>
/// Represents a request to the Ollama embeddings API endpoint.
/// </summary>
public record OllamaEmbeddingRequest
{
    /// <summary>
    /// Gets the name of the model to use for embedding generation.
    /// </summary>
    [JsonPropertyName("model")]
    public string Model { get; init; } = string.Empty;

    /// <summary>
    /// Gets the text to generate embeddings for.
    /// </summary>
    [JsonPropertyName("prompt")]
    public string Prompt { get; init; } = string.Empty;

    /// <summary>
    /// Gets the optional parameters to customize the embedding generation.
    /// </summary>
    [JsonPropertyName("options")]
    [JsonIgnore(Condition = JsonIgnoreCondition.WhenWritingNull)]
    public OllamaOptions? Options { get; init; }
}

/// <summary>
/// Represents a response from the Ollama embeddings API endpoint.
/// </summary>
public record OllamaEmbeddingResponse
{
    /// <summary>
    /// Gets the vector representation (embedding) of the input text.
    /// </summary>
    [JsonPropertyName("embedding")]
    public List<float> Embedding { get; init; } = new();
}

#endregion

#region Common Options

/// <summary>
/// Represents common options for customizing Ollama model behavior.
/// </summary>
public record OllamaOptions
{
    /// <summary>
    /// Gets the temperature parameter for controlling randomness in generation.
    /// </summary>
    /// <remarks>
    /// Higher values (e.g., 0.8) make output more random, lower values (e.g., 0.2) make it more deterministic.
    /// </remarks>
    [JsonPropertyName("temperature")]
    [JsonIgnore(Condition = JsonIgnoreCondition.WhenWritingNull)]
    public float? Temperature { get; init; }

    /// <summary>
    /// Gets the maximum number of tokens to generate (equivalent to max_tokens).
    /// </summary>
    [JsonPropertyName("num_predict")]
    [JsonIgnore(Condition = JsonIgnoreCondition.WhenWritingNull)]
    public int? NumPredict { get; init; }

    /// <summary>
    /// Gets the top-k sampling parameter.
    /// </summary>
    /// <remarks>
    /// Limits token selection to the top K most probable tokens.
    /// </remarks>
    [JsonPropertyName("top_k")]
    [JsonIgnore(Condition = JsonIgnoreCondition.WhenWritingNull)]
    public int? TopK { get; init; }

    /// <summary>
    /// Gets the top-p (nucleus) sampling parameter.
    /// </summary>
    /// <remarks>
    /// Limits token selection to a subset of tokens with a cumulative probability above P.
    /// </remarks>
    [JsonPropertyName("top_p")]
    [JsonIgnore(Condition = JsonIgnoreCondition.WhenWritingNull)]
    public float? TopP { get; init; }

    /// <summary>
    /// Gets the list of sequences that will stop generation when encountered.
    /// </summary>
    [JsonPropertyName("stop")]
    [JsonIgnore(Condition = JsonIgnoreCondition.WhenWritingNull)]
    public List<string>? Stop { get; init; }

    // Additional parameters like seed, mirostat, etc. can be added as needed
}

#endregion
