using System.Text.Json.Serialization;

namespace ConduitLLM.Core.Models;

/// <summary>
/// Represents a choice within a streaming chat completion chunk.
/// </summary>
public class StreamingChoice
{
    /// <summary>
    /// The index of the choice in the list of choices.
    /// </summary>
    [JsonPropertyName("index")]
    public int Index { get; set; }

    /// <summary>
    /// A chat completion delta generated by streamed model responses.
    /// </summary>
    [JsonPropertyName("delta")]
    public DeltaContent Delta { get; set; } = null!; // Initialized later during deserialization

    /// <summary>
    /// The reason the model stopped generating tokens. This will be "stop" if the model
    /// hit a natural stop point or a provided stop sequence, "length" if the maximum
    /// number of tokens specified in the request was reached, or null if the stream is ongoing.
    /// </summary>
    [JsonPropertyName("finish_reason")]
    [JsonIgnore(Condition = JsonIgnoreCondition.WhenWritingNull)]
    public string? FinishReason { get; set; }
}
