using System.Text.Json.Serialization;

namespace ConduitLLM.Core.Models;

/// <summary>
/// Represents a single choice within a chat completion response.
/// </summary>
public class Choice
{
    /// <summary>
    /// The reason the model stopped generating tokens. This will be "stop" if the model hit a natural stop point or a provided stop sequence,
    /// "length" if the maximum number of tokens specified in the request was reached,
    /// "content_filter" if content was omitted due to a flag from our content filters,
    /// "tool_calls" if the model called a tool, or "function_call" (deprecated) if the model called a function.
    /// </summary>
    [JsonPropertyName("finish_reason")]
    public required string FinishReason { get; set; }

    /// <summary>
    /// The index of the choice in the list of choices.
    /// </summary>
    [JsonPropertyName("index")]
    public required int Index { get; set; }

    /// <summary>
    /// A chat completion message generated by the model.
    /// </summary>
    [JsonPropertyName("message")]
    public required Message Message { get; set; }

    /// <summary>
    /// Log probability information for the choice. (Optional, depends on request parameter)
    /// </summary>
    [JsonPropertyName("logprobs")]
    [JsonIgnore(Condition = JsonIgnoreCondition.WhenWritingNull)]
    public object? Logprobs { get; set; } // Define a specific Logprobs class if needed
}

/// <summary>
/// Defines common reasons why a model might stop generating tokens.
/// </summary>
public static class FinishReason
{
    public const string Stop = "stop";
    public const string Length = "length";
    public const string ContentFilter = "content_filter";
    public const string ToolCalls = "tool_calls";
    public const string FunctionCall = "function_call"; // Deprecated
}
