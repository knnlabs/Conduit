"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[6116],{749:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>p,frontMatter:()=>a,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"audio/speech-to-text","title":"Speech-to-Text","description":"Comprehensive guide to speech-to-text transcription with multiple providers and advanced features","source":"@site/docs/audio/speech-to-text.md","sourceDirName":"audio","slug":"/audio/speech-to-text","permalink":"/Conduit/docs/audio/speech-to-text","draft":false,"unlisted":false,"editUrl":"https://github.com/knnlabs/Conduit/tree/main/website/docs/audio/speech-to-text.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Speech-to-Text","description":"Comprehensive guide to speech-to-text transcription with multiple providers and advanced features"}}');var r=t(4848),i=t(8453);const a={sidebar_position:2,title:"Speech-to-Text",description:"Comprehensive guide to speech-to-text transcription with multiple providers and advanced features"},o="Speech-to-Text",c={},l=[{value:"Quick Start",id:"quick-start",level:2},{value:"Basic Transcription",id:"basic-transcription",level:3},{value:"Advanced Transcription with Timestamps",id:"advanced-transcription-with-timestamps",level:3},{value:"Supported Models",id:"supported-models",level:2},{value:"OpenAI Whisper",id:"openai-whisper",level:3},{value:"Deepgram Nova-2",id:"deepgram-nova-2",level:3},{value:"Google Cloud Speech",id:"google-cloud-speech",level:3},{value:"Azure Speech Services",id:"azure-speech-services",level:3},{value:"Request Parameters",id:"request-parameters",level:2},{value:"Core Parameters",id:"core-parameters",level:3},{value:"Supported Audio Formats",id:"supported-audio-formats",level:3},{value:"Language Support",id:"language-support",level:3},{value:"Response Formats",id:"response-formats",level:2},{value:"Text Format (Default)",id:"text-format-default",level:3},{value:"JSON Format",id:"json-format",level:3},{value:"Verbose JSON with Timestamps",id:"verbose-json-with-timestamps",level:3},{value:"SRT Subtitle Format",id:"srt-subtitle-format",level:3},{value:"VTT Format",id:"vtt-format",level:3},{value:"Advanced Features",id:"advanced-features",level:2},{value:"Speaker Diarization",id:"speaker-diarization",level:3},{value:"Custom Vocabulary and Context",id:"custom-vocabulary-and-context",level:3},{value:"Noise Reduction and Enhancement",id:"noise-reduction-and-enhancement",level:3},{value:"Real-Time Streaming",id:"real-time-streaming",level:2},{value:"WebSocket Streaming",id:"websocket-streaming",level:3},{value:"Live Audio Capture",id:"live-audio-capture",level:3},{value:"Batch Processing",id:"batch-processing",level:2},{value:"Processing Multiple Files",id:"processing-multiple-files",level:3},{value:"Use Case Examples",id:"use-case-examples",level:2},{value:"Meeting Transcription",id:"meeting-transcription",level:3},{value:"Podcast Transcription with Chapters",id:"podcast-transcription-with-chapters",level:3},{value:"Error Handling and Troubleshooting",id:"error-handling-and-troubleshooting",level:2},{value:"Common Errors",id:"common-errors",level:3},{value:"Audio Quality Issues",id:"audio-quality-issues",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Caching Transcriptions",id:"caching-transcriptions",level:3},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"speech-to-text",children:"Speech-to-Text"})}),"\n",(0,r.jsx)(n.p,{children:"Conduit's speech-to-text capabilities provide high-accuracy transcription services with support for multiple providers, languages, and advanced features like speaker diarization, timestamps, and real-time streaming."}),"\n",(0,r.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,r.jsx)(n.h3,{id:"basic-transcription",children:"Basic Transcription"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:"import OpenAI from 'openai';\nimport fs from 'fs';\n\nconst openai = new OpenAI({\n  apiKey: 'condt_your_virtual_key',\n  baseURL: 'https://api.conduit.yourdomain.com/v1'\n});\n\nconst transcription = await openai.audio.transcriptions.create({\n  file: fs.createReadStream('meeting.mp3'),\n  model: 'whisper-1'\n});\n\nconsole.log(transcription.text);\n"})}),"\n",(0,r.jsx)(n.h3,{id:"advanced-transcription-with-timestamps",children:"Advanced Transcription with Timestamps"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:"const transcription = await openai.audio.transcriptions.create({\n  file: fs.createReadStream('interview.wav'),\n  model: 'whisper-1',\n  language: 'en',\n  response_format: 'verbose_json',\n  timestamp_granularities: ['word', 'segment'],\n  temperature: 0.0,\n  prompt: 'This is an interview about artificial intelligence and machine learning.'\n});\n\nconsole.log('Full text:', transcription.text);\nconsole.log('Word-level timestamps:', transcription.words);\nconsole.log('Segment timestamps:', transcription.segments);\n"})}),"\n",(0,r.jsx)(n.h2,{id:"supported-models",children:"Supported Models"}),"\n",(0,r.jsx)(n.h3,{id:"openai-whisper",children:"OpenAI Whisper"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsxs)(n.strong,{children:["Model: ",(0,r.jsx)(n.code,{children:"whisper-1"})]})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Languages"}),": 100+ languages supported"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Accuracy"}),": State-of-the-art for most languages"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Features"}),": Automatic language detection, punctuation, timestamps"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cost"}),": $0.006 per minute"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:"const transcription = await openai.audio.transcriptions.create({\n  file: audioFile,\n  model: 'whisper-1',\n  language: 'auto',  // Automatic language detection\n  temperature: 0.0   // More deterministic output\n});\n"})}),"\n",(0,r.jsx)(n.h3,{id:"deepgram-nova-2",children:"Deepgram Nova-2"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsxs)(n.strong,{children:["Model: ",(0,r.jsx)(n.code,{children:"deepgram-nova-2"})]})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Languages"}),": 30+ languages with high accuracy"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Speed"}),": Ultra-fast processing"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Features"}),": Smart formatting, speaker diarization, custom models"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cost"}),": $0.0043 per minute"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:"const transcription = await openai.audio.transcriptions.create({\n  file: audioFile,\n  model: 'deepgram-nova-2',\n  smart_format: true,\n  diarize: true,\n  punctuate: true,\n  utterances: true\n});\n"})}),"\n",(0,r.jsx)(n.h3,{id:"google-cloud-speech",children:"Google Cloud Speech"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsxs)(n.strong,{children:["Model: ",(0,r.jsx)(n.code,{children:"google-speech-latest"})]})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Languages"}),": 125+ languages and dialects"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Features"}),": Auto-punctuation, profanity filtering, speaker recognition"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Accuracy"}),": Excellent for diverse accents and dialects"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cost"}),": $0.004 per minute"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:"const transcription = await openai.audio.transcriptions.create({\n  file: audioFile,\n  model: 'google-speech-latest',\n  enable_automatic_punctuation: true,\n  enable_speaker_diarization: true,\n  diarization_speaker_count: 2\n});\n"})}),"\n",(0,r.jsx)(n.h3,{id:"azure-speech-services",children:"Azure Speech Services"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsxs)(n.strong,{children:["Model: ",(0,r.jsx)(n.code,{children:"azure-speech-universal"})]})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Languages"}),": 100+ languages"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Features"}),": Custom speech models, real-time transcription"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Specialization"}),": Enterprise scenarios, custom vocabularies"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cost"}),": $0.005 per minute"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:"const transcription = await openai.audio.transcriptions.create({\n  file: audioFile,\n  model: 'azure-speech-universal',\n  profanity_filter: 'masked',\n  add_word_level_timestamps: true,\n  add_diarization: true\n});\n"})}),"\n",(0,r.jsx)(n.h2,{id:"request-parameters",children:"Request Parameters"}),"\n",(0,r.jsx)(n.h3,{id:"core-parameters",children:"Core Parameters"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:"const transcription = await openai.audio.transcriptions.create({\n  // Required parameters\n  file: fs.createReadStream('audio.mp3'),    // Audio file\n  model: 'whisper-1',                        // Model selection\n  \n  // Optional parameters\n  language: 'en',                            // Language code\n  prompt: 'Context about the audio',         // Improve accuracy\n  response_format: 'verbose_json',           // Output format\n  temperature: 0.0,                          // Randomness (0.0-1.0)\n  timestamp_granularities: ['word']          // Timestamp detail\n});\n"})}),"\n",(0,r.jsx)(n.h3,{id:"supported-audio-formats",children:"Supported Audio Formats"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Format"}),(0,r.jsx)(n.th,{children:"Extension"}),(0,r.jsx)(n.th,{children:"Max Size"}),(0,r.jsx)(n.th,{children:"Quality"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"MP3"})}),(0,r.jsx)(n.td,{children:".mp3"}),(0,r.jsx)(n.td,{children:"100MB"}),(0,r.jsx)(n.td,{children:"Good compression"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"WAV"})}),(0,r.jsx)(n.td,{children:".wav"}),(0,r.jsx)(n.td,{children:"100MB"}),(0,r.jsx)(n.td,{children:"Uncompressed, high quality"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"M4A"})}),(0,r.jsx)(n.td,{children:".m4a"}),(0,r.jsx)(n.td,{children:"100MB"}),(0,r.jsx)(n.td,{children:"Apple format, good quality"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"FLAC"})}),(0,r.jsx)(n.td,{children:".flac"}),(0,r.jsx)(n.td,{children:"100MB"}),(0,r.jsx)(n.td,{children:"Lossless compression"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"OGG"})}),(0,r.jsx)(n.td,{children:".ogg"}),(0,r.jsx)(n.td,{children:"100MB"}),(0,r.jsx)(n.td,{children:"Open source format"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"WEBM"})}),(0,r.jsx)(n.td,{children:".webm"}),(0,r.jsx)(n.td,{children:"100MB"}),(0,r.jsx)(n.td,{children:"Web optimized"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"language-support",children:"Language Support"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Most Common Languages:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:"// Specify language for better accuracy\nconst languages = {\n  'en': 'English',\n  'es': 'Spanish', \n  'fr': 'French',\n  'de': 'German',\n  'it': 'Italian',\n  'pt': 'Portuguese',\n  'ru': 'Russian',\n  'ja': 'Japanese',\n  'ko': 'Korean',\n  'zh': 'Chinese',\n  'ar': 'Arabic',\n  'hi': 'Hindi'\n};\n\nconst transcription = await openai.audio.transcriptions.create({\n  file: audioFile,\n  model: 'whisper-1',\n  language: 'es'  // Spanish\n});\n"})}),"\n",(0,r.jsx)(n.h2,{id:"response-formats",children:"Response Formats"}),"\n",(0,r.jsx)(n.h3,{id:"text-format-default",children:"Text Format (Default)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:"const transcription = await openai.audio.transcriptions.create({\n  file: audioFile,\n  model: 'whisper-1',\n  response_format: 'text'\n});\n\n// Returns: \"Hello, this is a test transcription.\"\nconsole.log(transcription);\n"})}),"\n",(0,r.jsx)(n.h3,{id:"json-format",children:"JSON Format"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:"const transcription = await openai.audio.transcriptions.create({\n  file: audioFile,\n  model: 'whisper-1',\n  response_format: 'json'\n});\n\n// Returns: { \"text\": \"Hello, this is a test transcription.\" }\nconsole.log(transcription.text);\n"})}),"\n",(0,r.jsx)(n.h3,{id:"verbose-json-with-timestamps",children:"Verbose JSON with Timestamps"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:"const transcription = await openai.audio.transcriptions.create({\n  file: audioFile,\n  model: 'whisper-1',\n  response_format: 'verbose_json',\n  timestamp_granularities: ['word', 'segment']\n});\n\nconsole.log('Response format:', transcription);\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Verbose JSON Response:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'{\n  "task": "transcribe",\n  "language": "english",\n  "duration": 8.470000267028809,\n  "text": "The beach was a popular spot on a hot summer day. People were swimming in the ocean and building sandcastles in the sand.",\n  "words": [\n    {\n      "word": "The",\n      "start": 0.0,\n      "end": 0.23\n    },\n    {\n      "word": "beach",\n      "start": 0.23,\n      "end": 0.52\n    }\n  ],\n  "segments": [\n    {\n      "id": 0,\n      "seek": 0,\n      "start": 0.0,\n      "end": 8.470000267028809,\n      "text": " The beach was a popular spot on a hot summer day. People were swimming in the ocean and building sandcastles in the sand.",\n      "tokens": [1033, 7534, 390, 257, 2968, 4633, 319, 257, 3024, 3931, 1110, 13, 4380, 547, 10899, 287, 262, 9151, 290, 2615, 6450, 2701, 829, 287, 262, 6450, 13],\n      "temperature": 0.0,\n      "avg_logprob": -0.2860786020755768,\n      "compression_ratio": 1.2363636493682861,\n      "no_speech_prob": 0.00985979475080967\n    }\n  ]\n}\n'})}),"\n",(0,r.jsx)(n.h3,{id:"srt-subtitle-format",children:"SRT Subtitle Format"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:"const transcription = await openai.audio.transcriptions.create({\n  file: audioFile,\n  model: 'whisper-1',\n  response_format: 'srt'\n});\n\n// Returns SRT format:\n// 1\n// 00:00:00,000 --\x3e 00:00:04,000\n// The beach was a popular spot on a hot summer day.\n//\n// 2  \n// 00:00:04,000 --\x3e 00:00:08,470\n// People were swimming in the ocean and building sandcastles.\n"})}),"\n",(0,r.jsx)(n.h3,{id:"vtt-format",children:"VTT Format"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:"const transcription = await openai.audio.transcriptions.create({\n  file: audioFile,\n  model: 'whisper-1',\n  response_format: 'vtt'\n});\n\n// Returns WebVTT format for web players\n"})}),"\n",(0,r.jsx)(n.h2,{id:"advanced-features",children:"Advanced Features"}),"\n",(0,r.jsx)(n.h3,{id:"speaker-diarization",children:"Speaker Diarization"}),"\n",(0,r.jsx)(n.p,{children:"Identify different speakers in audio:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:"const transcription = await openai.audio.transcriptions.create({\n  file: audioFile,\n  model: 'deepgram-nova-2',\n  diarize: true,\n  utterances: true,\n  punctuate: true\n});\n\n// Response includes speaker labels\nconsole.log('Speakers identified:', transcription.metadata.speakers);\ntranscription.results.utterances.forEach(utterance => {\n  console.log(`Speaker ${utterance.speaker}: ${utterance.transcript}`);\n});\n"})}),"\n",(0,r.jsx)(n.h3,{id:"custom-vocabulary-and-context",children:"Custom Vocabulary and Context"}),"\n",(0,r.jsx)(n.p,{children:"Improve accuracy with context:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:"const transcription = await openai.audio.transcriptions.create({\n  file: audioFile,\n  model: 'whisper-1',\n  prompt: 'This is a technical discussion about machine learning, artificial intelligence, neural networks, and deep learning algorithms.',\n  temperature: 0.0  // More consistent with prompt\n});\n"})}),"\n",(0,r.jsx)(n.h3,{id:"noise-reduction-and-enhancement",children:"Noise Reduction and Enhancement"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:"// Some providers support audio enhancement\nconst transcription = await openai.audio.transcriptions.create({\n  file: audioFile,\n  model: 'deepgram-nova-2',\n  noise_reduction: true,\n  smart_format: true,\n  redact: ['pii'],  // Remove personally identifiable information\n  search: ['machine learning', 'AI', 'neural networks']  // Highlight keywords\n});\n"})}),"\n",(0,r.jsx)(n.h2,{id:"real-time-streaming",children:"Real-Time Streaming"}),"\n",(0,r.jsx)(n.h3,{id:"websocket-streaming",children:"WebSocket Streaming"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:"import WebSocket from 'ws';\n\nclass StreamingTranscriber {\n  constructor(apiKey) {\n    this.ws = new WebSocket('wss://api.conduit.yourdomain.com/v1/audio/stream/transcriptions', {\n      headers: {\n        'Authorization': `Bearer ${apiKey}`\n      }\n    });\n    \n    this.setupConnection();\n  }\n\n  setupConnection() {\n    this.ws.on('open', () => {\n      // Configure streaming session\n      this.ws.send(JSON.stringify({\n        type: 'configure',\n        config: {\n          model: 'deepgram-nova-2',\n          language: 'en-US',\n          smart_format: true,\n          interim_results: true,\n          punctuate: true,\n          diarize: true,\n          encoding: 'linear16',\n          sample_rate: 16000\n        }\n      }));\n    });\n\n    this.ws.on('message', (data) => {\n      const result = JSON.parse(data);\n      this.handleTranscriptionResult(result);\n    });\n\n    this.ws.on('error', (error) => {\n      console.error('WebSocket error:', error);\n    });\n  }\n\n  sendAudioChunk(audioBuffer) {\n    if (this.ws.readyState === WebSocket.OPEN) {\n      this.ws.send(audioBuffer);\n    }\n  }\n\n  handleTranscriptionResult(result) {\n    if (result.type === 'Results') {\n      const transcript = result.channel.alternatives[0].transcript;\n      \n      if (result.is_final) {\n        console.log('Final:', transcript);\n        this.onFinalTranscript(transcript);\n      } else {\n        console.log('Interim:', transcript);\n        this.onInterimTranscript(transcript);\n      }\n    }\n  }\n\n  onFinalTranscript(transcript) {\n    // Handle final transcript\n    document.getElementById('final-transcript').textContent += transcript + ' ';\n  }\n\n  onInterimTranscript(transcript) {\n    // Handle interim results\n    document.getElementById('interim-transcript').textContent = transcript;\n  }\n\n  close() {\n    this.ws.close();\n  }\n}\n"})}),"\n",(0,r.jsx)(n.h3,{id:"live-audio-capture",children:"Live Audio Capture"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:"class LiveAudioTranscriber {\n  constructor(apiKey) {\n    this.transcriber = new StreamingTranscriber(apiKey);\n    this.mediaRecorder = null;\n    this.audioStream = null;\n  }\n\n  async startRecording() {\n    try {\n      this.audioStream = await navigator.mediaDevices.getUserMedia({\n        audio: {\n          sampleRate: 16000,\n          channelCount: 1,\n          echoCancellation: true,\n          noiseSuppression: true\n        }\n      });\n\n      this.mediaRecorder = new MediaRecorder(this.audioStream, {\n        mimeType: 'audio/webm;codecs=opus'\n      });\n\n      this.mediaRecorder.ondataavailable = (event) => {\n        if (event.data.size > 0) {\n          this.transcriber.sendAudioChunk(event.data);\n        }\n      };\n\n      this.mediaRecorder.start(100); // Send data every 100ms\n    } catch (error) {\n      console.error('Error starting recording:', error);\n    }\n  }\n\n  stopRecording() {\n    if (this.mediaRecorder && this.mediaRecorder.state !== 'inactive') {\n      this.mediaRecorder.stop();\n    }\n    \n    if (this.audioStream) {\n      this.audioStream.getTracks().forEach(track => track.stop());\n    }\n    \n    this.transcriber.close();\n  }\n}\n"})}),"\n",(0,r.jsx)(n.h2,{id:"batch-processing",children:"Batch Processing"}),"\n",(0,r.jsx)(n.h3,{id:"processing-multiple-files",children:"Processing Multiple Files"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:"class BatchTranscriber {\n  constructor(conduitClient, concurrency = 5) {\n    this.client = conduitClient;\n    this.concurrency = concurrency;\n  }\n\n  async transcribeFiles(audioFiles, options = {}) {\n    const results = [];\n    \n    // Process files in batches to respect rate limits\n    for (let i = 0; i < audioFiles.length; i += this.concurrency) {\n      const batch = audioFiles.slice(i, i + this.concurrency);\n      \n      const batchPromises = batch.map(async (file, index) => {\n        try {\n          const transcription = await this.client.audio.transcriptions.create({\n            file: fs.createReadStream(file.path),\n            model: 'whisper-1',\n            response_format: 'verbose_json',\n            ...options\n          });\n\n          return {\n            file: file.path,\n            success: true,\n            transcription: transcription,\n            duration: transcription.duration,\n            wordCount: transcription.text.split(' ').length\n          };\n        } catch (error) {\n          return {\n            file: file.path,\n            success: false,\n            error: error.message\n          };\n        }\n      });\n\n      const batchResults = await Promise.allSettled(batchPromises);\n      results.push(...batchResults.map(result => result.value));\n      \n      // Optional delay between batches\n      if (i + this.concurrency < audioFiles.length) {\n        await new Promise(resolve => setTimeout(resolve, 1000));\n      }\n    }\n\n    return results;\n  }\n\n  generateReport(results) {\n    const successful = results.filter(r => r.success);\n    const failed = results.filter(r => !r.success);\n    \n    const totalDuration = successful.reduce((sum, r) => sum + r.duration, 0);\n    const totalWords = successful.reduce((sum, r) => sum + r.wordCount, 0);\n    \n    return {\n      totalFiles: results.length,\n      successful: successful.length,\n      failed: failed.length,\n      totalDuration: totalDuration,\n      totalWords: totalWords,\n      failedFiles: failed.map(f => ({ file: f.file, error: f.error }))\n    };\n  }\n}\n\n// Usage\nconst transcriber = new BatchTranscriber(openai);\nconst audioFiles = [\n  { path: 'meeting1.mp3' },\n  { path: 'interview2.wav' },\n  { path: 'podcast3.m4a' }\n];\n\nconst results = await transcriber.transcribeFiles(audioFiles, {\n  language: 'en',\n  temperature: 0.0\n});\n\nconst report = transcriber.generateReport(results);\nconsole.log('Transcription report:', report);\n"})}),"\n",(0,r.jsx)(n.h2,{id:"use-case-examples",children:"Use Case Examples"}),"\n",(0,r.jsx)(n.h3,{id:"meeting-transcription",children:"Meeting Transcription"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:"class MeetingTranscriber {\n  async transcribeMeeting(audioFile, meetingInfo = {}) {\n    const prompt = `This is a ${meetingInfo.type || 'business'} meeting ` +\n                  `with ${meetingInfo.participants || 'multiple participants'}. ` +\n                  `Topics include: ${meetingInfo.topics || 'general discussion'}.`;\n\n    const transcription = await openai.audio.transcriptions.create({\n      file: fs.createReadStream(audioFile),\n      model: 'deepgram-nova-2',\n      response_format: 'verbose_json',\n      diarize: true,\n      smart_format: true,\n      prompt: prompt\n    });\n\n    return {\n      fullTranscript: transcription.text,\n      speakers: this.extractSpeakers(transcription),\n      summary: await this.generateSummary(transcription.text),\n      actionItems: await this.extractActionItems(transcription.text),\n      duration: transcription.duration,\n      timestamp: new Date().toISOString()\n    };\n  }\n\n  extractSpeakers(transcription) {\n    // Extract speaker information from diarization\n    const speakers = new Set();\n    transcription.results?.utterances?.forEach(utterance => {\n      speakers.add(utterance.speaker);\n    });\n    return Array.from(speakers);\n  }\n\n  async generateSummary(transcript) {\n    const response = await openai.chat.completions.create({\n      model: 'gpt-4',\n      messages: [{\n        role: 'user',\n        content: `Please provide a concise summary of this meeting transcript:\\n\\n${transcript}`\n      }],\n      max_tokens: 500\n    });\n    \n    return response.choices[0].message.content;\n  }\n\n  async extractActionItems(transcript) {\n    const response = await openai.chat.completions.create({\n      model: 'gpt-4',\n      messages: [{\n        role: 'user',\n        content: `Extract action items from this meeting transcript. Format as a bullet list:\\n\\n${transcript}`\n      }],\n      max_tokens: 300\n    });\n    \n    return response.choices[0].message.content;\n  }\n}\n"})}),"\n",(0,r.jsx)(n.h3,{id:"podcast-transcription-with-chapters",children:"Podcast Transcription with Chapters"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:"class PodcastTranscriber {\n  async transcribePodcast(audioFile, showInfo = {}) {\n    const transcription = await openai.audio.transcriptions.create({\n      file: fs.createReadStream(audioFile),\n      model: 'whisper-1',\n      response_format: 'verbose_json',\n      timestamp_granularities: ['segment'],\n      prompt: `This is a podcast episode titled \"${showInfo.title}\". ` +\n              `Hosts: ${showInfo.hosts}. Topics: ${showInfo.topics}.`\n    });\n\n    const chapters = this.generateChapters(transcription.segments);\n    const transcript = this.formatTranscript(transcription);\n    \n    return {\n      metadata: {\n        title: showInfo.title,\n        duration: transcription.duration,\n        language: transcription.language,\n        processingTime: Date.now()\n      },\n      chapters: chapters,\n      fullTranscript: transcript,\n      searchableText: transcription.text,\n      srtSubtitles: this.generateSRT(transcription.segments)\n    };\n  }\n\n  generateChapters(segments, chapterLengthMinutes = 10) {\n    const chapterLength = chapterLengthMinutes * 60; // Convert to seconds\n    const chapters = [];\n    let currentChapter = {\n      title: 'Chapter 1',\n      startTime: 0,\n      segments: []\n    };\n\n    segments.forEach(segment => {\n      if (segment.start - currentChapter.startTime >= chapterLength) {\n        // Start new chapter\n        currentChapter.endTime = segment.start;\n        currentChapter.text = currentChapter.segments.map(s => s.text).join(' ');\n        chapters.push(currentChapter);\n\n        currentChapter = {\n          title: `Chapter ${chapters.length + 1}`,\n          startTime: segment.start,\n          segments: []\n        };\n      }\n      \n      currentChapter.segments.push(segment);\n    });\n\n    // Add final chapter\n    if (currentChapter.segments.length > 0) {\n      currentChapter.endTime = segments[segments.length - 1].end;\n      currentChapter.text = currentChapter.segments.map(s => s.text).join(' ');\n      chapters.push(currentChapter);\n    }\n\n    return chapters;\n  }\n\n  formatTranscript(transcription) {\n    return transcription.segments.map((segment, index) => {\n      const timestamp = this.formatTimestamp(segment.start);\n      return `[${timestamp}] ${segment.text.trim()}`;\n    }).join('\\n\\n');\n  }\n\n  formatTimestamp(seconds) {\n    const hours = Math.floor(seconds / 3600);\n    const minutes = Math.floor((seconds % 3600) / 60);\n    const secs = Math.floor(seconds % 60);\n    \n    if (hours > 0) {\n      return `${hours}:${minutes.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')}`;\n    } else {\n      return `${minutes}:${secs.toString().padStart(2, '0')}`;\n    }\n  }\n\n  generateSRT(segments) {\n    return segments.map((segment, index) => {\n      const startTime = this.formatSRTTimestamp(segment.start);\n      const endTime = this.formatSRTTimestamp(segment.end);\n      \n      return `${index + 1}\\n${startTime} --\x3e ${endTime}\\n${segment.text.trim()}\\n`;\n    }).join('\\n');\n  }\n\n  formatSRTTimestamp(seconds) {\n    const hours = Math.floor(seconds / 3600);\n    const minutes = Math.floor((seconds % 3600) / 60);\n    const secs = seconds % 60;\n    const milliseconds = Math.floor((secs % 1) * 1000);\n    const wholeSeconds = Math.floor(secs);\n    \n    return `${hours.toString().padStart(2, '0')}:${minutes.toString().padStart(2, '0')}:${wholeSeconds.toString().padStart(2, '0')},${milliseconds.toString().padStart(3, '0')}`;\n  }\n}\n"})}),"\n",(0,r.jsx)(n.h2,{id:"error-handling-and-troubleshooting",children:"Error Handling and Troubleshooting"}),"\n",(0,r.jsx)(n.h3,{id:"common-errors",children:"Common Errors"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:"try {\n  const transcription = await openai.audio.transcriptions.create({\n    file: fs.createReadStream('audio.mp3'),\n    model: 'whisper-1'\n  });\n} catch (error) {\n  switch (error.code) {\n    case 'invalid_file_format':\n      console.log('File format not supported. Use mp3, wav, m4a, etc.');\n      break;\n      \n    case 'file_too_large':\n      console.log('File exceeds 100MB limit. Consider splitting the audio.');\n      break;\n      \n    case 'audio_too_long':\n      console.log('Audio exceeds maximum duration. Split into shorter segments.');\n      break;\n      \n    case 'unsupported_language':\n      console.log('Language not supported by selected model.');\n      break;\n      \n    case 'provider_unavailable':\n      console.log('Transcription provider temporarily unavailable.');\n      // Try alternative provider\n      break;\n      \n    case 'rate_limit_exceeded':\n      console.log('Rate limit exceeded. Wait before retrying.');\n      break;\n      \n    default:\n      console.log('Transcription error:', error.message);\n  }\n}\n"})}),"\n",(0,r.jsx)(n.h3,{id:"audio-quality-issues",children:"Audio Quality Issues"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:"class AudioQualityChecker {\n  static analyzeAudio(audioFile) {\n    // This would typically use an audio analysis library\n    const analysis = {\n      sampleRate: 44100,\n      bitRate: 128,\n      duration: 300,\n      channels: 2,\n      format: 'mp3'\n    };\n    \n    const recommendations = [];\n    \n    if (analysis.sampleRate < 16000) {\n      recommendations.push('Consider using higher sample rate (16kHz+) for better accuracy');\n    }\n    \n    if (analysis.bitRate < 64) {\n      recommendations.push('Low bit rate may affect transcription quality');\n    }\n    \n    if (analysis.channels > 1) {\n      recommendations.push('Mono audio often works better for transcription');\n    }\n    \n    return { analysis, recommendations };\n  }\n}\n"})}),"\n",(0,r.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,r.jsx)(n.h3,{id:"caching-transcriptions",children:"Caching Transcriptions"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:"class CachedTranscriber {\n  constructor(conduitClient, cacheDir = './transcription-cache') {\n    this.client = conduitClient;\n    this.cacheDir = cacheDir;\n    this.ensureCacheDir();\n  }\n\n  ensureCacheDir() {\n    if (!fs.existsSync(this.cacheDir)) {\n      fs.mkdirSync(this.cacheDir, { recursive: true });\n    }\n  }\n\n  getCacheKey(audioFile, options) {\n    const stats = fs.statSync(audioFile);\n    const content = `${audioFile}-${stats.mtime.getTime()}-${JSON.stringify(options)}`;\n    return require('crypto').createHash('md5').update(content).digest('hex');\n  }\n\n  async transcribeWithCache(audioFile, options = {}) {\n    const cacheKey = this.getCacheKey(audioFile, options);\n    const cacheFile = path.join(this.cacheDir, `${cacheKey}.json`);\n    \n    // Check cache first\n    if (fs.existsSync(cacheFile)) {\n      console.log('Using cached transcription');\n      return JSON.parse(fs.readFileSync(cacheFile, 'utf8'));\n    }\n    \n    // Transcribe and cache\n    const transcription = await this.client.audio.transcriptions.create({\n      file: fs.createReadStream(audioFile),\n      ...options\n    });\n    \n    fs.writeFileSync(cacheFile, JSON.stringify(transcription, null, 2));\n    console.log('Transcription cached');\n    \n    return transcription;\n  }\n}\n"})}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Text-to-Speech"}),": Convert text to natural speech with ",(0,r.jsx)(n.a,{href:"text-to-speech",children:"voice synthesis"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Real-Time Audio"}),": Build interactive voice applications with ",(0,r.jsx)(n.a,{href:"real-time-audio",children:"real-time audio"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Audio Providers"}),": Compare capabilities across ",(0,r.jsx)(n.a,{href:"providers",children:"different providers"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Integration Examples"}),": See complete ",(0,r.jsx)(n.a,{href:"../clients/overview",children:"integration patterns"})]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>o});var s=t(6540);const r={},i=s.createContext(r);function a(e){const n=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);