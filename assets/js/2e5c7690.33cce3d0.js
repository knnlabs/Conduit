"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[2236],{873:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>m,frontMatter:()=>l,metadata:()=>o,toc:()=>r});const o=JSON.parse('{"id":"core-apis/models","title":"Models","description":"Discover available models, capabilities, and pricing across all providers","source":"@site/docs/core-apis/models.md","sourceDirName":"core-apis","slug":"/core-apis/models","permalink":"/Conduit/docs/core-apis/models","draft":false,"unlisted":false,"editUrl":"https://github.com/knnlabs/Conduit/tree/main/website/docs/core-apis/models.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Models","description":"Discover available models, capabilities, and pricing across all providers"}}');var i=t(4848),s=t(8453);const l={sidebar_position:4,title:"Models",description:"Discover available models, capabilities, and pricing across all providers"},a="Models",d={},r=[{value:"Quick Start",id:"quick-start",level:2},{value:"List All Available Models",id:"list-all-available-models",level:3},{value:"Get Specific Model Details",id:"get-specific-model-details",level:3},{value:"Model Response Format",id:"model-response-format",level:2},{value:"Standard Model Object",id:"standard-model-object",level:3},{value:"Model Categories and Capabilities",id:"model-categories-and-capabilities",level:2},{value:"Chat Completion Models",id:"chat-completion-models",level:3},{value:"Embedding Models",id:"embedding-models",level:3},{value:"Audio Models",id:"audio-models",level:3},{value:"Image Generation Models",id:"image-generation-models",level:3},{value:"Advanced Model Filtering",id:"advanced-model-filtering",level:2},{value:"Filter by Capabilities",id:"filter-by-capabilities",level:3},{value:"Cost Comparison Tool",id:"cost-comparison-tool",level:3},{value:"Model Status and Health",id:"model-status-and-health",level:2},{value:"Check Model Availability",id:"check-model-availability",level:3},{value:"Model Recommendation Engine",id:"model-recommendation-engine",level:3},{value:"Real-Time Model Updates",id:"real-time-model-updates",level:2},{value:"Model Change Notifications",id:"model-change-notifications",level:3},{value:"Integration Patterns",id:"integration-patterns",level:2},{value:"Dynamic Model Selection",id:"dynamic-model-selection",level:3},{value:"Next Steps",id:"next-steps",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"models",children:"Models"})}),"\n",(0,i.jsx)(n.p,{children:"The Models endpoint provides comprehensive information about available models across all providers in your Conduit deployment, including capabilities, pricing, context limits, and real-time availability status."}),"\n",(0,i.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,i.jsx)(n.h3,{id:"list-all-available-models",children:"List All Available Models"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"import OpenAI from 'openai';\n\nconst openai = new OpenAI({\n  apiKey: 'condt_your_virtual_key',\n  baseURL: 'https://api.conduit.yourdomain.com/v1'\n});\n\nconst models = await openai.models.list();\n\nconsole.log('Available models:', models.data.length);\n\nmodels.data.forEach(model => {\n  console.log(`${model.id} (${model.owned_by})`);\n  console.log(`  Capabilities: ${model.capabilities?.join(', ')}`);\n  console.log(`  Context: ${model.context_length} tokens`);\n  console.log(`  Status: ${model.status}`);\n});\n"})}),"\n",(0,i.jsx)(n.h3,{id:"get-specific-model-details",children:"Get Specific Model Details"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"const modelDetails = await openai.models.retrieve('gpt-4');\n\nconsole.log('Model:', modelDetails.id);\nconsole.log('Provider:', modelDetails.owned_by);\nconsole.log('Description:', modelDetails.description);\nconsole.log('Capabilities:', modelDetails.capabilities);\nconsole.log('Pricing:', modelDetails.pricing);\nconsole.log('Context Length:', modelDetails.context_length);\nconsole.log('Max Output:', modelDetails.max_output_tokens);\n"})}),"\n",(0,i.jsx)(n.h2,{id:"model-response-format",children:"Model Response Format"}),"\n",(0,i.jsx)(n.h3,{id:"standard-model-object",children:"Standard Model Object"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "id": "gpt-4",\n  "object": "model",\n  "created": 1687882411,\n  "owned_by": "openai",\n  "status": "available",\n  "description": "GPT-4 is a large multimodal model that can solve difficult problems with greater accuracy than previous models.",\n  "capabilities": [\n    "chat_completions",\n    "function_calling", \n    "vision",\n    "streaming"\n  ],\n  "context_length": 8192,\n  "max_output_tokens": 4096,\n  "pricing": {\n    "input_cost_per_token": 0.00003,\n    "output_cost_per_token": 0.00006,\n    "currency": "USD",\n    "per_unit": 1\n  },\n  "limits": {\n    "requests_per_minute": 10000,\n    "tokens_per_minute": 300000\n  },\n  "multimodal": {\n    "supports_images": true,\n    "supports_audio": false,\n    "max_image_size": "20MB",\n    "supported_formats": ["png", "jpg", "gif", "webp"]\n  },\n  "provider_specific": {\n    "temperature_range": [0.0, 2.0],\n    "supports_system_messages": true,\n    "supports_tools": true\n  }\n}\n'})}),"\n",(0,i.jsx)(n.h2,{id:"model-categories-and-capabilities",children:"Model Categories and Capabilities"}),"\n",(0,i.jsx)(n.h3,{id:"chat-completion-models",children:"Chat Completion Models"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"// Filter models by capability\nconst chatModels = models.data.filter(model => \n  model.capabilities?.includes('chat_completions')\n);\n\nconsole.log('Chat Completion Models:');\nchatModels.forEach(model => {\n  console.log(`- ${model.id}: ${model.context_length} context, $${model.pricing?.input_cost_per_token * 1000}/1K tokens`);\n});\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Major Chat Models:"})}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Provider"}),(0,i.jsx)(n.th,{children:"Model"}),(0,i.jsx)(n.th,{children:"Context"}),(0,i.jsx)(n.th,{children:"Cost (per 1K tokens)"}),(0,i.jsx)(n.th,{children:"Capabilities"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"OpenAI"})}),(0,i.jsx)(n.td,{children:"gpt-4o"}),(0,i.jsx)(n.td,{children:"128K"}),(0,i.jsx)(n.td,{children:"$5.00 / $15.00"}),(0,i.jsx)(n.td,{children:"Vision, function calling, streaming"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"OpenAI"})}),(0,i.jsx)(n.td,{children:"gpt-4-turbo"}),(0,i.jsx)(n.td,{children:"128K"}),(0,i.jsx)(n.td,{children:"$10.00 / $30.00"}),(0,i.jsx)(n.td,{children:"Vision, function calling, JSON mode"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"OpenAI"})}),(0,i.jsx)(n.td,{children:"gpt-3.5-turbo"}),(0,i.jsx)(n.td,{children:"16K"}),(0,i.jsx)(n.td,{children:"$0.50 / $1.50"}),(0,i.jsx)(n.td,{children:"Function calling, streaming"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Anthropic"})}),(0,i.jsx)(n.td,{children:"claude-3-opus"}),(0,i.jsx)(n.td,{children:"200K"}),(0,i.jsx)(n.td,{children:"$15.00 / $75.00"}),(0,i.jsx)(n.td,{children:"Vision, tool use, large context"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Anthropic"})}),(0,i.jsx)(n.td,{children:"claude-3-sonnet"}),(0,i.jsx)(n.td,{children:"200K"}),(0,i.jsx)(n.td,{children:"$3.00 / $15.00"}),(0,i.jsx)(n.td,{children:"Vision, tool use, balanced"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Google"})}),(0,i.jsx)(n.td,{children:"gemini-pro"}),(0,i.jsx)(n.td,{children:"32K"}),(0,i.jsx)(n.td,{children:"$0.50 / $1.50"}),(0,i.jsx)(n.td,{children:"Multimodal, function calling"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Cohere"})}),(0,i.jsx)(n.td,{children:"command-r-plus"}),(0,i.jsx)(n.td,{children:"128K"}),(0,i.jsx)(n.td,{children:"$3.00 / $15.00"}),(0,i.jsx)(n.td,{children:"RAG optimized, tool use"})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"embedding-models",children:"Embedding Models"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"const embeddingModels = models.data.filter(model => \n  model.capabilities?.includes('embeddings')\n);\n\nconsole.log('Embedding Models:');\nembeddingModels.forEach(model => {\n  const dimensions = model.provider_specific?.dimensions || 'Variable';\n  console.log(`- ${model.id}: ${dimensions} dimensions, $${model.pricing?.input_cost_per_token * 1000}/1K tokens`);\n});\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Major Embedding Models:"})}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Provider"}),(0,i.jsx)(n.th,{children:"Model"}),(0,i.jsx)(n.th,{children:"Dimensions"}),(0,i.jsx)(n.th,{children:"Context"}),(0,i.jsx)(n.th,{children:"Cost (per 1K tokens)"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"OpenAI"})}),(0,i.jsx)(n.td,{children:"text-embedding-3-large"}),(0,i.jsx)(n.td,{children:"3072"}),(0,i.jsx)(n.td,{children:"8191"}),(0,i.jsx)(n.td,{children:"$0.13"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"OpenAI"})}),(0,i.jsx)(n.td,{children:"text-embedding-3-small"}),(0,i.jsx)(n.td,{children:"1536"}),(0,i.jsx)(n.td,{children:"8191"}),(0,i.jsx)(n.td,{children:"$0.02"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"OpenAI"})}),(0,i.jsx)(n.td,{children:"text-embedding-ada-002"}),(0,i.jsx)(n.td,{children:"1536"}),(0,i.jsx)(n.td,{children:"8191"}),(0,i.jsx)(n.td,{children:"$0.10"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Cohere"})}),(0,i.jsx)(n.td,{children:"embed-english-v3.0"}),(0,i.jsx)(n.td,{children:"1024"}),(0,i.jsx)(n.td,{children:"512"}),(0,i.jsx)(n.td,{children:"$0.10"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Google"})}),(0,i.jsx)(n.td,{children:"textembedding-gecko"}),(0,i.jsx)(n.td,{children:"768"}),(0,i.jsx)(n.td,{children:"3072"}),(0,i.jsx)(n.td,{children:"$0.025"})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"audio-models",children:"Audio Models"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"const audioModels = models.data.filter(model => \n  model.capabilities?.some(cap => cap.includes('audio'))\n);\n\nconsole.log('Audio Models:');\naudioModels.forEach(model => {\n  console.log(`- ${model.id}: ${model.capabilities.filter(cap => cap.includes('audio')).join(', ')}`);\n});\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Audio Capabilities:"})}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Provider"}),(0,i.jsx)(n.th,{children:"Model"}),(0,i.jsx)(n.th,{children:"Capabilities"}),(0,i.jsx)(n.th,{children:"Languages"}),(0,i.jsx)(n.th,{children:"Cost"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"OpenAI"})}),(0,i.jsx)(n.td,{children:"whisper-1"}),(0,i.jsx)(n.td,{children:"speech_to_text"}),(0,i.jsx)(n.td,{children:"99+ languages"}),(0,i.jsx)(n.td,{children:"$0.006/minute"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"OpenAI"})}),(0,i.jsx)(n.td,{children:"tts-1"}),(0,i.jsx)(n.td,{children:"text_to_speech"}),(0,i.jsx)(n.td,{children:"Optimized for English"}),(0,i.jsx)(n.td,{children:"$15/1M chars"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"OpenAI"})}),(0,i.jsx)(n.td,{children:"tts-1-hd"}),(0,i.jsx)(n.td,{children:"text_to_speech"}),(0,i.jsx)(n.td,{children:"High quality"}),(0,i.jsx)(n.td,{children:"$15/1M chars"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"ElevenLabs"})}),(0,i.jsx)(n.td,{children:"eleven-turbo-v2"}),(0,i.jsx)(n.td,{children:"text_to_speech"}),(0,i.jsx)(n.td,{children:"29+ languages"}),(0,i.jsx)(n.td,{children:"$0.18/1K chars"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Deepgram"})}),(0,i.jsx)(n.td,{children:"nova-2"}),(0,i.jsx)(n.td,{children:"speech_to_text"}),(0,i.jsx)(n.td,{children:"Multiple languages"}),(0,i.jsx)(n.td,{children:"$0.0043/minute"})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"image-generation-models",children:"Image Generation Models"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"const imageModels = models.data.filter(model => \n  model.capabilities?.includes('image_generation')\n);\n\nconsole.log('Image Generation Models:');\nimageModels.forEach(model => {\n  const resolutions = model.provider_specific?.supported_sizes || [];\n  console.log(`- ${model.id}: ${resolutions.join(', ')}`);\n});\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Image Generation Capabilities:"})}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Provider"}),(0,i.jsx)(n.th,{children:"Model"}),(0,i.jsx)(n.th,{children:"Max Resolution"}),(0,i.jsx)(n.th,{children:"Features"}),(0,i.jsx)(n.th,{children:"Cost per Image"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"OpenAI"})}),(0,i.jsx)(n.td,{children:"dall-e-3"}),(0,i.jsx)(n.td,{children:"1024x1024"}),(0,i.jsx)(n.td,{children:"HD quality, style control"}),(0,i.jsx)(n.td,{children:"$0.04-0.08"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"OpenAI"})}),(0,i.jsx)(n.td,{children:"dall-e-2"}),(0,i.jsx)(n.td,{children:"1024x1024"}),(0,i.jsx)(n.td,{children:"Standard quality"}),(0,i.jsx)(n.td,{children:"$0.02"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"MiniMax"})}),(0,i.jsx)(n.td,{children:"minimax-image"}),(0,i.jsx)(n.td,{children:"1024x1024"}),(0,i.jsx)(n.td,{children:"Fast generation"}),(0,i.jsx)(n.td,{children:"$0.01-0.04"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Replicate"})}),(0,i.jsx)(n.td,{children:"Various"}),(0,i.jsx)(n.td,{children:"Up to 2048x2048"}),(0,i.jsx)(n.td,{children:"Model variety"}),(0,i.jsx)(n.td,{children:"$0.001-0.10"})]})]})]}),"\n",(0,i.jsx)(n.h2,{id:"advanced-model-filtering",children:"Advanced Model Filtering"}),"\n",(0,i.jsx)(n.h3,{id:"filter-by-capabilities",children:"Filter by Capabilities"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"class ModelDiscovery {\n  constructor(apiKey) {\n    this.apiKey = apiKey;\n    this.openai = new OpenAI({\n      apiKey: apiKey,\n      baseURL: 'https://api.conduit.yourdomain.com/v1'\n    });\n    this.models = null;\n  }\n\n  async loadModels() {\n    if (!this.models) {\n      const response = await this.openai.models.list();\n      this.models = response.data;\n    }\n    return this.models;\n  }\n\n  async findModelsByCapability(capability) {\n    const models = await this.loadModels();\n    return models.filter(model => \n      model.capabilities?.includes(capability)\n    );\n  }\n\n  async findModelsByProvider(provider) {\n    const models = await this.loadModels();\n    return models.filter(model => \n      model.owned_by.toLowerCase() === provider.toLowerCase()\n    );\n  }\n\n  async findModelsByPriceRange(maxInputCost, maxOutputCost = null) {\n    const models = await this.loadModels();\n    return models.filter(model => {\n      const pricing = model.pricing;\n      if (!pricing) return false;\n      \n      const inputOk = pricing.input_cost_per_token <= maxInputCost;\n      const outputOk = !maxOutputCost || \n        pricing.output_cost_per_token <= maxOutputCost;\n      \n      return inputOk && outputOk;\n    });\n  }\n\n  async findModelsByContextLength(minContext) {\n    const models = await this.loadModels();\n    return models.filter(model => \n      model.context_length >= minContext\n    );\n  }\n\n  async findBestModelsForTask(task) {\n    const models = await this.loadModels();\n    \n    const taskRequirements = {\n      'chat': { capabilities: ['chat_completions'] },\n      'reasoning': { \n        capabilities: ['chat_completions'], \n        providers: ['openai', 'anthropic'],\n        minContext: 8000 \n      },\n      'code': { \n        capabilities: ['chat_completions'], \n        providers: ['openai', 'anthropic', 'cohere'],\n        minContext: 16000 \n      },\n      'search': { capabilities: ['embeddings'] },\n      'transcription': { capabilities: ['speech_to_text'] },\n      'voice': { capabilities: ['text_to_speech'] },\n      'images': { capabilities: ['image_generation'] },\n      'vision': { \n        capabilities: ['chat_completions'],\n        multimodal: true \n      }\n    };\n\n    const requirements = taskRequirements[task];\n    if (!requirements) return [];\n\n    let filtered = models;\n\n    // Filter by capabilities\n    if (requirements.capabilities) {\n      filtered = filtered.filter(model =>\n        requirements.capabilities.every(cap =>\n          model.capabilities?.includes(cap)\n        )\n      );\n    }\n\n    // Filter by providers\n    if (requirements.providers) {\n      filtered = filtered.filter(model =>\n        requirements.providers.includes(model.owned_by.toLowerCase())\n      );\n    }\n\n    // Filter by context length\n    if (requirements.minContext) {\n      filtered = filtered.filter(model =>\n        model.context_length >= requirements.minContext\n      );\n    }\n\n    // Filter by multimodal support\n    if (requirements.multimodal) {\n      filtered = filtered.filter(model =>\n        model.multimodal?.supports_images\n      );\n    }\n\n    // Sort by cost efficiency (input cost per token)\n    return filtered.sort((a, b) => {\n      const costA = a.pricing?.input_cost_per_token || Infinity;\n      const costB = b.pricing?.input_cost_per_token || Infinity;\n      return costA - costB;\n    });\n  }\n}\n\n// Usage examples\nconst discovery = new ModelDiscovery('condt_your_virtual_key');\n\n// Find vision-capable models\nconst visionModels = await discovery.findBestModelsForTask('vision');\nconsole.log('Vision Models:', visionModels.map(m => m.id));\n\n// Find cheap embedding models\nconst cheapEmbeddings = await discovery.findModelsByPriceRange(0.001);\nconsole.log('Affordable Embeddings:', cheapEmbeddings.map(m => m.id));\n\n// Find large context models\nconst largeContext = await discovery.findModelsByContextLength(100000);\nconsole.log('Large Context Models:', largeContext.map(m => `${m.id} (${m.context_length})`));\n"})}),"\n",(0,i.jsx)(n.h3,{id:"cost-comparison-tool",children:"Cost Comparison Tool"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"class ModelCostComparison {\n  constructor(apiKey) {\n    this.apiKey = apiKey;\n    this.openai = new OpenAI({\n      apiKey: apiKey,\n      baseURL: 'https://api.conduit.yourdomain.com/v1'\n    });\n  }\n\n  async compareModelsForWorkload(workload) {\n    const models = await this.openai.models.list();\n    const chatModels = models.data.filter(m => \n      m.capabilities?.includes('chat_completions')\n    );\n\n    const estimates = chatModels.map(model => {\n      const pricing = model.pricing;\n      if (!pricing) return null;\n\n      const inputCost = workload.inputTokens * pricing.input_cost_per_token;\n      const outputCost = workload.outputTokens * pricing.output_cost_per_token;\n      const totalCost = (inputCost + outputCost) * workload.requestsPerDay * 30; // Monthly\n\n      return {\n        model: model.id,\n        provider: model.owned_by,\n        context: model.context_length,\n        dailyCost: (inputCost + outputCost) * workload.requestsPerDay,\n        monthlyCost: totalCost,\n        costPerRequest: inputCost + outputCost,\n        costBreakdown: {\n          input: inputCost * workload.requestsPerDay * 30,\n          output: outputCost * workload.requestsPerDay * 30\n        }\n      };\n    }).filter(Boolean);\n\n    return estimates.sort((a, b) => a.monthlyCost - b.monthlyCost);\n  }\n\n  formatCostComparison(estimates) {\n    console.log('\\n\ud83d\udcca Model Cost Comparison (Monthly)\\n');\n    console.log('Rank | Model | Provider | Context | Monthly Cost | Per Request');\n    console.log('-----|-------|----------|---------|--------------|------------');\n\n    estimates.forEach((estimate, index) => {\n      const rank = (index + 1).toString().padStart(4);\n      const model = estimate.model.padEnd(20);\n      const provider = estimate.provider.padEnd(10);\n      const context = (estimate.context + 'K').padEnd(8);\n      const monthly = ('$' + estimate.monthlyCost.toFixed(2)).padStart(11);\n      const perRequest = ('$' + estimate.costPerRequest.toFixed(4)).padStart(10);\n\n      console.log(`${rank} | ${model} | ${provider} | ${context} | ${monthly} | ${perRequest}`);\n    });\n  }\n}\n\n// Usage example\nconst costComparison = new ModelCostComparison('condt_your_virtual_key');\n\nconst workload = {\n  inputTokens: 1000,    // Average input per request\n  outputTokens: 500,    // Average output per request\n  requestsPerDay: 1000  // Daily request volume\n};\n\nconst estimates = await costComparison.compareModelsForWorkload(workload);\ncostComparison.formatCostComparison(estimates.slice(0, 10)); // Top 10 cheapest\n"})}),"\n",(0,i.jsx)(n.h2,{id:"model-status-and-health",children:"Model Status and Health"}),"\n",(0,i.jsx)(n.h3,{id:"check-model-availability",children:"Check Model Availability"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"async function checkModelHealth() {\n  const models = await openai.models.list();\n  \n  const healthReport = {\n    total: models.data.length,\n    available: 0,\n    unavailable: 0,\n    limited: 0,\n    byProvider: {}\n  };\n\n  models.data.forEach(model => {\n    // Count by status\n    switch (model.status) {\n      case 'available':\n        healthReport.available++;\n        break;\n      case 'unavailable':\n        healthReport.unavailable++;\n        break;\n      case 'limited':\n        healthReport.limited++;\n        break;\n    }\n\n    // Group by provider\n    const provider = model.owned_by;\n    if (!healthReport.byProvider[provider]) {\n      healthReport.byProvider[provider] = {\n        total: 0,\n        available: 0,\n        unavailable: 0,\n        limited: 0\n      };\n    }\n    \n    healthReport.byProvider[provider].total++;\n    healthReport.byProvider[provider][model.status]++;\n  });\n\n  return healthReport;\n}\n\n// Generate health report\nconst health = await checkModelHealth();\n\nconsole.log('\ud83d\udfe2 Model Health Report');\nconsole.log(`Total Models: ${health.total}`);\nconsole.log(`Available: ${health.available}`);\nconsole.log(`Unavailable: ${health.unavailable}`);\nconsole.log(`Limited: ${health.limited}`);\n\nconsole.log('\\n\ud83d\udce1 By Provider:');\nObject.entries(health.byProvider).forEach(([provider, stats]) => {\n  const availability = (stats.available / stats.total * 100).toFixed(1);\n  console.log(`${provider}: ${stats.available}/${stats.total} (${availability}% available)`);\n});\n"})}),"\n",(0,i.jsx)(n.h3,{id:"model-recommendation-engine",children:"Model Recommendation Engine"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"class ModelRecommendationEngine {\n  constructor(apiKey) {\n    this.apiKey = apiKey;\n    this.openai = new OpenAI({\n      apiKey: apiKey,\n      baseURL: 'https://api.conduit.yourdomain.com/v1'\n    });\n  }\n\n  async recommendModels(requirements) {\n    const models = await this.openai.models.list();\n    let candidates = models.data.filter(m => m.status === 'available');\n\n    // Apply filters based on requirements\n    if (requirements.capability) {\n      candidates = candidates.filter(m => \n        m.capabilities?.includes(requirements.capability)\n      );\n    }\n\n    if (requirements.maxCostPerToken) {\n      candidates = candidates.filter(m => \n        m.pricing?.input_cost_per_token <= requirements.maxCostPerToken\n      );\n    }\n\n    if (requirements.minContextLength) {\n      candidates = candidates.filter(m => \n        m.context_length >= requirements.minContextLength\n      );\n    }\n\n    if (requirements.multimodal) {\n      candidates = candidates.filter(m => \n        m.multimodal?.supports_images\n      );\n    }\n\n    if (requirements.preferredProviders) {\n      candidates = candidates.filter(m => \n        requirements.preferredProviders.includes(m.owned_by.toLowerCase())\n      );\n    }\n\n    // Score and rank candidates\n    const scored = candidates.map(model => ({\n      ...model,\n      score: this.calculateScore(model, requirements)\n    }));\n\n    return scored\n      .sort((a, b) => b.score - a.score)\n      .slice(0, requirements.maxResults || 5);\n  }\n\n  calculateScore(model, requirements) {\n    let score = 100; // Base score\n\n    // Cost efficiency (lower cost = higher score)\n    if (model.pricing?.input_cost_per_token) {\n      const costFactor = 1 / (model.pricing.input_cost_per_token * 10000);\n      score += Math.min(costFactor, 50); // Cap cost bonus\n    }\n\n    // Context length bonus\n    if (requirements.minContextLength) {\n      const contextBonus = Math.min(\n        (model.context_length / requirements.minContextLength - 1) * 20,\n        30\n      );\n      score += contextBonus;\n    }\n\n    // Provider preference\n    if (requirements.preferredProviders?.includes(model.owned_by.toLowerCase())) {\n      score += 25;\n    }\n\n    // Capability richness\n    const capabilityBonus = (model.capabilities?.length || 0) * 5;\n    score += Math.min(capabilityBonus, 25);\n\n    // Quality indicators\n    if (model.id.includes('turbo') || model.id.includes('gpt-4')) {\n      score += 15; // OpenAI quality bonus\n    }\n    \n    if (model.id.includes('claude-3')) {\n      score += 15; // Anthropic quality bonus\n    }\n\n    // Penalize if limited availability\n    if (model.status === 'limited') {\n      score -= 20;\n    }\n\n    return Math.round(score);\n  }\n\n  explainRecommendation(model, requirements) {\n    const explanation = [`Recommended: ${model.id}`];\n    \n    explanation.push(`Provider: ${model.owned_by}`);\n    explanation.push(`Score: ${model.score}/100`);\n    \n    if (model.pricing) {\n      const monthlyCost = model.pricing.input_cost_per_token * 1000 * \n        (requirements.estimatedTokensPerMonth || 1000000);\n      explanation.push(`Estimated monthly cost: $${monthlyCost.toFixed(2)}`);\n    }\n\n    explanation.push(`Context: ${model.context_length.toLocaleString()} tokens`);\n    explanation.push(`Capabilities: ${model.capabilities?.join(', ')}`);\n\n    return explanation.join('\\n');\n  }\n}\n\n// Usage examples\nconst recommender = new ModelRecommendationEngine('condt_your_virtual_key');\n\n// Find best chat model for coding tasks\nconst codingModels = await recommender.recommendModels({\n  capability: 'chat_completions',\n  minContextLength: 16000,\n  maxCostPerToken: 0.00005,\n  preferredProviders: ['openai', 'anthropic'],\n  estimatedTokensPerMonth: 2000000,\n  maxResults: 3\n});\n\nconsole.log('\ud83e\udd16 Best Models for Coding:');\ncodingModels.forEach(model => {\n  console.log('\\n' + recommender.explainRecommendation(model, {\n    estimatedTokensPerMonth: 2000000\n  }));\n});\n\n// Find best embedding model for search\nconst embeddingModels = await recommender.recommendModels({\n  capability: 'embeddings',\n  maxCostPerToken: 0.0001,\n  maxResults: 3\n});\n\nconsole.log('\\n\ud83d\udd0d Best Embedding Models:');\nembeddingModels.forEach(model => {\n  console.log(`${model.id}: $${(model.pricing?.input_cost_per_token * 1000).toFixed(3)}/1K tokens`);\n});\n"})}),"\n",(0,i.jsx)(n.h2,{id:"real-time-model-updates",children:"Real-Time Model Updates"}),"\n",(0,i.jsx)(n.h3,{id:"model-change-notifications",children:"Model Change Notifications"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"// Using SignalR for real-time model updates\nimport { HubConnectionBuilder } from '@microsoft/signalr';\n\nclass ModelStatusMonitor {\n  constructor(apiKey) {\n    this.apiKey = apiKey;\n    this.connection = new HubConnectionBuilder()\n      .withUrl('https://api.conduit.yourdomain.com/hubs/navigation-state', {\n        accessTokenFactory: () => apiKey\n      })\n      .build();\n    \n    this.setupEventHandlers();\n  }\n\n  setupEventHandlers() {\n    this.connection.on('ModelStatusChanged', (data) => {\n      console.log(`Model ${data.modelId} status changed: ${data.oldStatus} \u2192 ${data.newStatus}`);\n      this.onModelStatusChange(data);\n    });\n\n    this.connection.on('NewModelAdded', (data) => {\n      console.log(`New model available: ${data.modelId} (${data.provider})`);\n      this.onNewModel(data);\n    });\n\n    this.connection.on('ModelRemoved', (data) => {\n      console.log(`Model removed: ${data.modelId}`);\n      this.onModelRemoved(data);\n    });\n\n    this.connection.on('ProviderHealthChanged', (data) => {\n      console.log(`Provider ${data.provider} health: ${data.status}`);\n      this.onProviderHealthChange(data);\n    });\n  }\n\n  async start() {\n    try {\n      await this.connection.start();\n      console.log('\ud83d\udd04 Connected to model status updates');\n    } catch (error) {\n      console.error('Failed to connect:', error);\n    }\n  }\n\n  onModelStatusChange(data) {\n    if (data.newStatus === 'unavailable') {\n      this.handleModelUnavailable(data.modelId);\n    } else if (data.newStatus === 'available' && data.oldStatus === 'unavailable') {\n      this.handleModelRestored(data.modelId);\n    }\n  }\n\n  onNewModel(data) {\n    // Automatically evaluate new models for existing use cases\n    this.evaluateNewModel(data);\n  }\n\n  handleModelUnavailable(modelId) {\n    // Implement fallback logic\n    console.log(`\u26a0\ufe0f  Model ${modelId} unavailable - switching to fallback`);\n  }\n\n  handleModelRestored(modelId) {\n    console.log(`\u2705 Model ${modelId} restored`);\n  }\n\n  async evaluateNewModel(modelData) {\n    // Check if new model might be better for current use cases\n    if (modelData.capabilities?.includes('chat_completions')) {\n      console.log(`\ud83c\udd95 New chat model available: ${modelData.modelId}`);\n      // Could trigger re-evaluation of model recommendations\n    }\n  }\n}\n\n// Usage\nconst monitor = new ModelStatusMonitor('condt_your_virtual_key');\nawait monitor.start();\n"})}),"\n",(0,i.jsx)(n.h2,{id:"integration-patterns",children:"Integration Patterns"}),"\n",(0,i.jsx)(n.h3,{id:"dynamic-model-selection",children:"Dynamic Model Selection"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"class AdaptiveModelSelector {\n  constructor(apiKey) {\n    this.apiKey = apiKey;\n    this.openai = new OpenAI({\n      apiKey: apiKey,\n      baseURL: 'https://api.conduit.yourdomain.com/v1'\n    });\n    this.modelCache = null;\n    this.lastUpdate = null;\n  }\n\n  async getAvailableModels(forceRefresh = false) {\n    const cacheAge = this.lastUpdate ? Date.now() - this.lastUpdate : Infinity;\n    \n    if (forceRefresh || !this.modelCache || cacheAge > 300000) { // 5 minutes\n      const response = await this.openai.models.list();\n      this.modelCache = response.data.filter(m => m.status === 'available');\n      this.lastUpdate = Date.now();\n    }\n    \n    return this.modelCache;\n  }\n\n  async selectBestModel(task, constraints = {}) {\n    const models = await this.getAvailableModels();\n    \n    const suitable = models.filter(model => {\n      // Check capability requirements\n      if (task.requiredCapabilities) {\n        const hasCapabilities = task.requiredCapabilities.every(cap =>\n          model.capabilities?.includes(cap)\n        );\n        if (!hasCapabilities) return false;\n      }\n\n      // Check cost constraints\n      if (constraints.maxCostPerToken) {\n        const cost = model.pricing?.input_cost_per_token || 0;\n        if (cost > constraints.maxCostPerToken) return false;\n      }\n\n      // Check context requirements\n      if (task.contextLength) {\n        if (model.context_length < task.contextLength) return false;\n      }\n\n      // Check provider preferences\n      if (constraints.preferredProviders) {\n        if (!constraints.preferredProviders.includes(model.owned_by)) return false;\n      }\n\n      return true;\n    });\n\n    if (suitable.length === 0) {\n      throw new Error('No suitable models available for this task');\n    }\n\n    // Select based on strategy\n    const strategy = constraints.selectionStrategy || 'balanced';\n    \n    switch (strategy) {\n      case 'cheapest':\n        return suitable.sort((a, b) => \n          (a.pricing?.input_cost_per_token || 0) - (b.pricing?.input_cost_per_token || 0)\n        )[0];\n        \n      case 'fastest':\n        // Prefer models known for speed\n        const fastModels = suitable.filter(m => \n          m.id.includes('turbo') || m.id.includes('3.5') || m.id.includes('haiku')\n        );\n        return fastModels.length > 0 ? fastModels[0] : suitable[0];\n        \n      case 'highest_quality':\n        // Prefer premium models\n        const qualityOrder = ['gpt-4', 'claude-3-opus', 'claude-3-sonnet', 'gpt-3.5'];\n        for (const prefix of qualityOrder) {\n          const found = suitable.find(m => m.id.startsWith(prefix));\n          if (found) return found;\n        }\n        return suitable[0];\n        \n      case 'balanced':\n      default:\n        // Balance cost and quality\n        return suitable.sort((a, b) => {\n          const scoreA = this.calculateBalancedScore(a);\n          const scoreB = this.calculateBalancedScore(b);\n          return scoreB - scoreA;\n        })[0];\n    }\n  }\n\n  calculateBalancedScore(model) {\n    let score = 100;\n    \n    // Cost efficiency (inverse cost)\n    const cost = model.pricing?.input_cost_per_token || 0;\n    score += Math.max(0, 50 - (cost * 100000)); // Scale cost to reasonable range\n    \n    // Context length bonus\n    score += Math.min(model.context_length / 1000, 50);\n    \n    // Capability richness\n    score += (model.capabilities?.length || 0) * 5;\n    \n    // Quality heuristics\n    if (model.id.includes('gpt-4')) score += 30;\n    if (model.id.includes('claude-3')) score += 25;\n    if (model.id.includes('turbo')) score += 10;\n    \n    return score;\n  }\n\n  async executeWithFallback(task, primaryModel = null) {\n    const models = primaryModel \n      ? [primaryModel] \n      : [await this.selectBestModel(task)];\n    \n    // Add fallback models\n    const allModels = await this.getAvailableModels();\n    const fallbacks = allModels\n      .filter(m => \n        !models.includes(m) && \n        task.requiredCapabilities?.every(cap => m.capabilities?.includes(cap))\n      )\n      .slice(0, 2); // Up to 2 fallbacks\n    \n    models.push(...fallbacks);\n\n    for (const model of models) {\n      try {\n        console.log(`\ud83c\udfaf Trying model: ${model.id}`);\n        \n        const result = await this.executeTask(task, model);\n        \n        console.log(`\u2705 Success with ${model.id}`);\n        return result;\n        \n      } catch (error) {\n        console.log(`\u274c Failed with ${model.id}: ${error.message}`);\n        \n        if (models.indexOf(model) === models.length - 1) {\n          throw new Error(`All models failed. Last error: ${error.message}`);\n        }\n      }\n    }\n  }\n\n  async executeTask(task, model) {\n    switch (task.type) {\n      case 'chat':\n        return await this.openai.chat.completions.create({\n          model: model.id,\n          messages: task.messages,\n          ...task.options\n        });\n        \n      case 'embedding':\n        return await this.openai.embeddings.create({\n          model: model.id,\n          input: task.input,\n          ...task.options\n        });\n        \n      default:\n        throw new Error(`Unsupported task type: ${task.type}`);\n    }\n  }\n}\n\n// Usage example\nconst selector = new AdaptiveModelSelector('condt_your_virtual_key');\n\nconst chatTask = {\n  type: 'chat',\n  requiredCapabilities: ['chat_completions'],\n  contextLength: 4000,\n  messages: [\n    { role: 'user', content: 'Explain quantum computing' }\n  ],\n  options: { max_tokens: 500 }\n};\n\nconst result = await selector.executeWithFallback(chatTask);\nconsole.log('Response:', result.choices[0].message.content);\n"})}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Chat Completions"}),": Use discovered models for ",(0,i.jsx)(n.a,{href:"chat-completions",children:"chat completions"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Embeddings"}),": Apply model selection to ",(0,i.jsx)(n.a,{href:"embeddings",children:"embedding generation"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Audio Platform"}),": Explore ",(0,i.jsx)(n.a,{href:"../audio/overview",children:"audio model capabilities"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Media Generation"}),": Discover ",(0,i.jsx)(n.a,{href:"../media/overview",children:"image and video models"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Integration Examples"}),": See complete ",(0,i.jsx)(n.a,{href:"../clients/overview",children:"client patterns"})]}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>l,x:()=>a});var o=t(6540);const i={},s=o.createContext(i);function l(e){const n=o.useContext(s);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:l(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);