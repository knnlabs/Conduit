"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[5765],{3388:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"audio/text-to-speech","title":"Text-to-Speech","description":"Comprehensive guide to text-to-speech synthesis with multiple providers and voice options","source":"@site/docs/audio/text-to-speech.md","sourceDirName":"audio","slug":"/audio/text-to-speech","permalink":"/Conduit/docs/audio/text-to-speech","draft":false,"unlisted":false,"editUrl":"https://github.com/knnlabs/Conduit/tree/main/website/docs/audio/text-to-speech.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Text-to-Speech","description":"Comprehensive guide to text-to-speech synthesis with multiple providers and voice options"},"sidebar":"docsSidebar","previous":{"title":"Core APIs Overview","permalink":"/Conduit/docs/core-apis/overview"},"next":{"title":"Real-Time Audio","permalink":"/Conduit/docs/audio/real-time-audio"}}');var i=t(4848),o=t(8453);const a={sidebar_position:3,title:"Text-to-Speech",description:"Comprehensive guide to text-to-speech synthesis with multiple providers and voice options"},r="Text-to-Speech",c={},l=[{value:"Quick Start",id:"quick-start",level:2},{value:"Basic Text-to-Speech",id:"basic-text-to-speech",level:3},{value:"Advanced TTS with Voice Control",id:"advanced-tts-with-voice-control",level:3},{value:"Supported Models and Providers",id:"supported-models-and-providers",level:2},{value:"OpenAI TTS",id:"openai-tts",level:3},{value:"ElevenLabs TTS",id:"elevenlabs-tts",level:3},{value:"Azure Speech Services",id:"azure-speech-services",level:3},{value:"Google Cloud Text-to-Speech",id:"google-cloud-text-to-speech",level:3},{value:"Request Parameters",id:"request-parameters",level:2},{value:"Core Parameters",id:"core-parameters",level:3},{value:"Supported Output Formats",id:"supported-output-formats",level:3},{value:"Language Support",id:"language-support",level:3},{value:"Advanced Features",id:"advanced-features",level:2},{value:"Voice Parameters",id:"voice-parameters",level:3},{value:"SSML Support",id:"ssml-support",level:3},{value:"Multilingual Speech",id:"multilingual-speech",level:3},{value:"Streaming Text-to-Speech",id:"streaming-text-to-speech",level:2},{value:"Real-Time Speech Synthesis",id:"real-time-speech-synthesis",level:3},{value:"Chunked Processing for Long Texts",id:"chunked-processing-for-long-texts",level:3},{value:"Use Case Examples",id:"use-case-examples",level:2},{value:"Audiobook Generation",id:"audiobook-generation",level:3},{value:"Interactive Voice Response (IVR)",id:"interactive-voice-response-ivr",level:3},{value:"Podcast/Content Creation",id:"podcastcontent-creation",level:3},{value:"Error Handling and Optimization",id:"error-handling-and-optimization",level:2},{value:"Common TTS Errors",id:"common-tts-errors",level:3},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"Integration with Real-Time Communication",id:"integration-with-real-time-communication",level:2},{value:"WebSocket TTS for Real-Time Applications",id:"websocket-tts-for-real-time-applications",level:3},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"text-to-speech",children:"Text-to-Speech"})}),"\n",(0,i.jsx)(n.p,{children:"Conduit's text-to-speech capabilities provide high-quality voice synthesis with support for multiple providers and voices through a unified OpenAI-compatible API."}),"\n",(0,i.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,i.jsx)(n.h3,{id:"basic-text-to-speech",children:"Basic Text-to-Speech"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"import OpenAI from 'openai';\nimport fs from 'fs';\n\nconst openai = new OpenAI({\n  apiKey: 'condt_your_virtual_key',\n  baseURL: 'https://api.conduit.yourdomain.com/v1'\n});\n\nconst mp3 = await openai.audio.speech.create({\n  model: 'tts-1',\n  voice: 'alloy',\n  input: 'Hello world! This is a demonstration of text-to-speech synthesis.'\n});\n\nconst buffer = Buffer.from(await mp3.arrayBuffer());\nfs.writeFileSync('speech.mp3', buffer);\n"})}),"\n",(0,i.jsx)(n.h3,{id:"advanced-tts-with-voice-control",children:"Advanced TTS with Voice Control"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"const speech = await openai.audio.speech.create({\n  model: 'elevenlabs-tts',\n  voice: 'rachel',\n  input: 'Welcome to our premium text-to-speech service. This voice has emotional control and natural intonation.',\n  response_format: 'mp3',\n  speed: 1.0,\n  // ElevenLabs-specific parameters\n  stability: 0.5,\n  similarity_boost: 0.5,\n  style: 0.2,\n  use_speaker_boost: true\n});\n\nconst audioBuffer = Buffer.from(await speech.arrayBuffer());\nfs.writeFileSync('premium-speech.mp3', audioBuffer);\n"})}),"\n",(0,i.jsx)(n.h2,{id:"supported-models-and-providers",children:"Supported Models and Providers"}),"\n",(0,i.jsx)(n.h3,{id:"openai-tts",children:"OpenAI TTS"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsxs)(n.strong,{children:["Models: ",(0,i.jsx)(n.code,{children:"tts-1"}),", ",(0,i.jsx)(n.code,{children:"tts-1-hd"})]})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Quality"}),": Standard and HD options"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Voices"}),": 6 built-in voices (alloy, echo, fable, onyx, nova, shimmer)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Languages"}),": Optimized for English, supports others"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cost"}),": $15.00 per 1M characters"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Latency"}),": 200-500ms for short texts"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"const speech = await openai.audio.speech.create({\n  model: 'tts-1-hd',          // HD quality\n  voice: 'nova',              // Female voice\n  input: 'Your text here',\n  response_format: 'mp3',\n  speed: 1.0                  // 0.25 to 4.0\n});\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Available Voices:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"alloy"}),": Neutral, balanced tone"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"echo"}),": Clear, professional male voice"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"fable"}),": Warm, storytelling voice"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"onyx"}),": Deep, authoritative male voice"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"nova"}),": Expressive female voice"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"shimmer"}),": Bright, energetic female voice"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"elevenlabs-tts",children:"ElevenLabs TTS"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsxs)(n.strong,{children:["Models: ",(0,i.jsx)(n.code,{children:"elevenlabs-tts"}),", ",(0,i.jsx)(n.code,{children:"elevenlabs-tts-v2"})]})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Quality"}),": Premium quality with emotional control"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Voices"}),": 1000+ voices including custom clones"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Languages"}),": 29+ languages"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cost"}),": $0.18-0.30 per 1K characters"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Features"}),": Voice cloning, emotional control, accent preservation"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"const speech = await openai.audio.speech.create({\n  model: 'elevenlabs-tts',\n  voice: 'rachel',            // Premium voice\n  input: 'This text will be spoken with natural emotion and intonation.',\n  response_format: 'mp3',\n  // ElevenLabs advanced parameters\n  stability: 0.5,             // 0.0-1.0, voice consistency\n  similarity_boost: 0.5,      // 0.0-1.0, voice similarity\n  style: 0.2,                 // 0.0-1.0, emotional range\n  use_speaker_boost: true     // Enhance clarity\n});\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Popular ElevenLabs Voices:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"rachel"}),": Natural female voice, American accent"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"drew"}),": Professional male voice"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"clyde"}),": Warm, friendly male voice"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"bella"}),": Expressive female voice"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"antoni"}),": Narrator-style male voice"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"elli"}),": Young, energetic female voice"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"azure-speech-services",children:"Azure Speech Services"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsxs)(n.strong,{children:["Models: ",(0,i.jsx)(n.code,{children:"azure-neural-tts"})]})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Quality"}),": High-quality neural voices"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Voices"}),": 400+ voices in 140+ languages"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Languages"}),": Comprehensive global coverage"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cost"}),": $4.00 per 1M characters"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Features"}),": SSML support, custom neural voices"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"const speech = await openai.audio.speech.create({\n  model: 'azure-neural-tts',\n  voice: 'en-US-JennyNeural',\n  input: 'Azure provides high-quality neural text-to-speech.',\n  response_format: 'wav',\n  language: 'en-US',\n  // Azure-specific parameters\n  pitch: '+0Hz',\n  rate: '1.0',\n  volume: '100'\n});\n"})}),"\n",(0,i.jsx)(n.h3,{id:"google-cloud-text-to-speech",children:"Google Cloud Text-to-Speech"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsxs)(n.strong,{children:["Models: ",(0,i.jsx)(n.code,{children:"google-wavenet"}),", ",(0,i.jsx)(n.code,{children:"google-neural2"})]})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Quality"}),": WaveNet and Neural2 technologies"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Voices"}),": 380+ voices in 40+ languages"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Languages"}),": Excellent multilingual support"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cost"}),": $4.00 per 1M characters"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Features"}),": Custom voice training, SSML"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"const speech = await openai.audio.speech.create({\n  model: 'google-neural2',\n  voice: 'en-US-Neural2-F',\n  input: 'Google Neural2 provides natural-sounding speech.',\n  response_format: 'ogg',\n  language: 'en-US',\n  // Google-specific parameters\n  speaking_rate: 1.0,\n  pitch: 0.0,\n  volume_gain_db: 0.0\n});\n"})}),"\n",(0,i.jsx)(n.h2,{id:"request-parameters",children:"Request Parameters"}),"\n",(0,i.jsx)(n.h3,{id:"core-parameters",children:"Core Parameters"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"const speech = await openai.audio.speech.create({\n  // Required parameters\n  model: 'tts-1',                    // TTS model selection\n  voice: 'alloy',                    // Voice selection\n  input: 'Text to synthesize',       // Input text (max 4096 chars)\n  \n  // Optional parameters\n  response_format: 'mp3',            // Output format\n  speed: 1.0,                        // Speech rate (0.25-4.0)\n  \n  // Provider-specific parameters\n  language: 'en-US',                 // Language code\n  pitch: 0.0,                        // Voice pitch adjustment\n  volume: 1.0,                       // Audio volume\n  emotion: 'neutral'                 // Emotional tone\n});\n"})}),"\n",(0,i.jsx)(n.h3,{id:"supported-output-formats",children:"Supported Output Formats"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Format"}),(0,i.jsx)(n.th,{children:"Extension"}),(0,i.jsx)(n.th,{children:"Quality"}),(0,i.jsx)(n.th,{children:"Compression"}),(0,i.jsx)(n.th,{children:"Use Case"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"MP3"})}),(0,i.jsx)(n.td,{children:".mp3"}),(0,i.jsx)(n.td,{children:"Good"}),(0,i.jsx)(n.td,{children:"High"}),(0,i.jsx)(n.td,{children:"Web streaming, mobile"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"OPUS"})}),(0,i.jsx)(n.td,{children:".opus"}),(0,i.jsx)(n.td,{children:"Excellent"}),(0,i.jsx)(n.td,{children:"High"}),(0,i.jsx)(n.td,{children:"Real-time communication"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"AAC"})}),(0,i.jsx)(n.td,{children:".aac"}),(0,i.jsx)(n.td,{children:"Good"}),(0,i.jsx)(n.td,{children:"Medium"}),(0,i.jsx)(n.td,{children:"Apple devices, streaming"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"FLAC"})}),(0,i.jsx)(n.td,{children:".flac"}),(0,i.jsx)(n.td,{children:"Excellent"}),(0,i.jsx)(n.td,{children:"Lossless"}),(0,i.jsx)(n.td,{children:"High-quality archival"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"WAV"})}),(0,i.jsx)(n.td,{children:".wav"}),(0,i.jsx)(n.td,{children:"Excellent"}),(0,i.jsx)(n.td,{children:"None"}),(0,i.jsx)(n.td,{children:"Professional audio"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"PCM"})}),(0,i.jsx)(n.td,{children:".pcm"}),(0,i.jsx)(n.td,{children:"Raw"}),(0,i.jsx)(n.td,{children:"None"}),(0,i.jsx)(n.td,{children:"Real-time processing"})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"language-support",children:"Language Support"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Top Languages by Provider:"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"OpenAI TTS:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Primary"}),": English (US, UK, AU)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Secondary"}),": Spanish, French, German, Italian, Portuguese, Russian, Japanese, Korean, Chinese"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"ElevenLabs:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Native"}),": English, Spanish, French, German, Italian, Portuguese, Polish, Ukrainian, Russian"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Supported"}),": 29+ languages with accent preservation"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Azure Speech:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Full Support"}),": 140+ languages and dialects"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Neural Voices"}),": 80+ languages"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Google Cloud:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"WaveNet"}),": 40+ languages"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Standard"}),": 220+ voices across 40+ languages"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"advanced-features",children:"Advanced Features"}),"\n",(0,i.jsx)(n.h3,{id:"voice-parameters",children:"Voice Parameters"}),"\n",(0,i.jsx)(n.p,{children:"For providers that support it, you can adjust voice characteristics:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"// ElevenLabs emotional parameters\nconst emotionalSpeech = await openai.audio.speech.create({\n  model: 'elevenlabs-tts',\n  voice: 'rachel',\n  input: 'I am feeling excited about this new technology!',\n  stability: 0.3,        // Lower = more variable/emotional\n  similarity_boost: 0.8, // Higher = closer to original voice\n  style: 0.7,           // Higher = more expressive\n  emotion: 'excited',    // Some providers support direct emotion control\n  energy_level: 'high'   // Energy/enthusiasm level\n});\n"})}),"\n",(0,i.jsx)(n.h3,{id:"ssml-support",children:"SSML Support"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:'// Speech Synthesis Markup Language for fine control\nconst ssmlText = `\n<speak>\n  <prosody rate="slow" pitch="low">\n    This text is spoken slowly with a low pitch.\n  </prosody>\n  <break time="1s"/>\n  <prosody rate="fast" pitch="high">\n    This text is spoken quickly with a high pitch.\n  </prosody>\n  <emphasis level="strong">\n    This text is emphasized strongly.\n  </emphasis>\n</speak>\n`;\n\nconst speech = await openai.audio.speech.create({\n  model: \'azure-neural-tts\',\n  voice: \'en-US-JennyNeural\',\n  input: ssmlText,\n  input_format: \'ssml\'\n});\n'})}),"\n",(0,i.jsx)(n.h3,{id:"multilingual-speech",children:"Multilingual Speech"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"// Automatic language detection and appropriate voice selection\nconst multilingualSpeech = async (texts) => {\n  const speeches = [];\n  \n  for (const { text, language } of texts) {\n    const voiceMap = {\n      'en': 'en-US-JennyNeural',\n      'es': 'es-ES-ElviraNeural', \n      'fr': 'fr-FR-DeniseNeural',\n      'de': 'de-DE-KatjaNeural',\n      'ja': 'ja-JP-NanamiNeural',\n      'zh': 'zh-CN-XiaoxiaoNeural'\n    };\n    \n    const speech = await openai.audio.speech.create({\n      model: 'azure-neural-tts',\n      voice: voiceMap[language] || voiceMap['en'],\n      input: text,\n      language: language\n    });\n    \n    speeches.push(await speech.arrayBuffer());\n  }\n  \n  return speeches;\n};\n\n// Usage\nconst multilingual = await multilingualSpeech([\n  { text: 'Hello, how are you?', language: 'en' },\n  { text: 'Hola, \xbfc\xf3mo est\xe1s?', language: 'es' },\n  { text: 'Bonjour, comment allez-vous?', language: 'fr' }\n]);\n"})}),"\n",(0,i.jsx)(n.h2,{id:"streaming-text-to-speech",children:"Streaming Text-to-Speech"}),"\n",(0,i.jsx)(n.h3,{id:"real-time-speech-synthesis",children:"Real-Time Speech Synthesis"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"import WebSocket from 'ws';\n\nclass StreamingTTS {\n  constructor(apiKey) {\n    this.apiKey = apiKey;\n    this.ws = null;\n    this.audioChunks = [];\n  }\n\n  async startStream(voice = 'alloy', format = 'pcm') {\n    this.ws = new WebSocket('wss://api.conduit.yourdomain.com/v1/audio/stream/speech', {\n      headers: {\n        'Authorization': `Bearer ${this.apiKey}`\n      }\n    });\n\n    this.ws.on('open', () => {\n      // Configure streaming session\n      this.ws.send(JSON.stringify({\n        type: 'configure',\n        config: {\n          model: 'tts-1',\n          voice: voice,\n          response_format: format,\n          chunk_size: 1024\n        }\n      }));\n    });\n\n    this.ws.on('message', (data) => {\n      if (data instanceof Buffer) {\n        // Audio chunk received\n        this.audioChunks.push(data);\n        this.onAudioChunk(data);\n      } else {\n        // Control message\n        const message = JSON.parse(data);\n        this.handleControlMessage(message);\n      }\n    });\n\n    this.ws.on('error', (error) => {\n      console.error('Streaming TTS error:', error);\n    });\n  }\n\n  sendText(text) {\n    if (this.ws && this.ws.readyState === WebSocket.OPEN) {\n      this.ws.send(JSON.stringify({\n        type: 'synthesize',\n        text: text\n      }));\n    }\n  }\n\n  onAudioChunk(audioData) {\n    // Play audio chunk immediately\n    this.playAudioChunk(audioData);\n  }\n\n  playAudioChunk(audioData) {\n    // Implement audio playback (browser or Node.js)\n    // This would integrate with Web Audio API or audio libraries\n    console.log(`Received audio chunk: ${audioData.length} bytes`);\n  }\n\n  handleControlMessage(message) {\n    switch (message.type) {\n      case 'synthesis_started':\n        console.log('Synthesis started');\n        break;\n      case 'synthesis_completed':\n        console.log('Synthesis completed');\n        break;\n      case 'error':\n        console.error('Synthesis error:', message.error);\n        break;\n    }\n  }\n\n  close() {\n    if (this.ws) {\n      this.ws.close();\n    }\n  }\n}\n\n// Usage\nconst streamingTTS = new StreamingTTS('condt_your_virtual_key');\nawait streamingTTS.startStream('nova', 'pcm');\n\n// Send text for immediate synthesis\nstreamingTTS.sendText('Hello, this will be spoken immediately!');\nstreamingTTS.sendText(' This continues the speech seamlessly.');\n"})}),"\n",(0,i.jsx)(n.h3,{id:"chunked-processing-for-long-texts",children:"Chunked Processing for Long Texts"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"class LongTextTTS {\n  constructor(apiKey) {\n    this.apiKey = apiKey;\n    this.maxChunkLength = 4000; // Stay under API limits\n  }\n\n  splitText(text) {\n    const sentences = text.match(/[^\\.!?]+[\\.!?]+/g) || [text];\n    const chunks = [];\n    let currentChunk = '';\n\n    for (const sentence of sentences) {\n      if (currentChunk.length + sentence.length > this.maxChunkLength) {\n        if (currentChunk) {\n          chunks.push(currentChunk.trim());\n          currentChunk = sentence;\n        } else {\n          // Single sentence too long, force split\n          chunks.push(sentence.trim());\n        }\n      } else {\n        currentChunk += sentence;\n      }\n    }\n\n    if (currentChunk) {\n      chunks.push(currentChunk.trim());\n    }\n\n    return chunks;\n  }\n\n  async synthesizeLongText(text, options = {}) {\n    const chunks = this.splitText(text);\n    const audioChunks = [];\n    \n    console.log(`Processing ${chunks.length} chunks...`);\n\n    for (let i = 0; i < chunks.length; i++) {\n      console.log(`Synthesizing chunk ${i + 1}/${chunks.length}`);\n      \n      try {\n        const speech = await openai.audio.speech.create({\n          model: 'tts-1',\n          voice: 'alloy',\n          input: chunks[i],\n          response_format: 'mp3',\n          ...options\n        });\n\n        const audioBuffer = Buffer.from(await speech.arrayBuffer());\n        audioChunks.push(audioBuffer);\n        \n        // Optional delay to respect rate limits\n        if (i < chunks.length - 1) {\n          await new Promise(resolve => setTimeout(resolve, 100));\n        }\n      } catch (error) {\n        console.error(`Error synthesizing chunk ${i + 1}:`, error);\n        throw error;\n      }\n    }\n\n    return this.concatenateAudio(audioChunks);\n  }\n\n  concatenateAudio(audioChunks) {\n    // Simple buffer concatenation (works for most formats)\n    return Buffer.concat(audioChunks);\n  }\n}\n\n// Usage\nconst longTTS = new LongTextTTS('condt_your_virtual_key');\n\nconst longText = `\n  This is a very long piece of text that needs to be converted to speech.\n  It contains multiple sentences and paragraphs that exceed the typical\n  character limits of text-to-speech APIs. The system will automatically\n  split this into appropriate chunks and synthesize each part separately,\n  then combine them into a single audio file.\n`;\n\nconst audioBuffer = await longTTS.synthesizeLongText(longText, {\n  voice: 'nova',\n  speed: 1.1\n});\n\nfs.writeFileSync('long-speech.mp3', audioBuffer);\n"})}),"\n",(0,i.jsx)(n.h2,{id:"use-case-examples",children:"Use Case Examples"}),"\n",(0,i.jsx)(n.h3,{id:"audiobook-generation",children:"Audiobook Generation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"class AudiobookGenerator {\n  constructor(apiKey) {\n    this.apiKey = apiKey;\n    this.chapterBreakDuration = 2000; // 2 seconds silence\n  }\n\n  async generateAudiobook(chapters, options = {}) {\n    const audioChapters = [];\n    \n    for (let i = 0; i < chapters.length; i++) {\n      console.log(`Generating chapter ${i + 1}: ${chapters[i].title}`);\n      \n      // Generate chapter introduction\n      const chapterIntro = `Chapter ${i + 1}: ${chapters[i].title}`;\n      const introAudio = await this.synthesizeText(chapterIntro, {\n        voice: 'onyx',  // Different voice for chapter titles\n        speed: 0.9,\n        ...options\n      });\n      \n      // Generate chapter content\n      const contentAudio = await this.synthesizeText(chapters[i].content, {\n        voice: 'nova',  // Main narration voice\n        speed: 1.0,\n        ...options\n      });\n      \n      // Add chapter break\n      const silenceBuffer = this.generateSilence(this.chapterBreakDuration);\n      \n      audioChapters.push(introAudio, contentAudio, silenceBuffer);\n    }\n    \n    return Buffer.concat(audioChapters);\n  }\n\n  async synthesizeText(text, options) {\n    const speech = await openai.audio.speech.create({\n      model: 'tts-1-hd',\n      input: text,\n      response_format: 'wav',\n      ...options\n    });\n    \n    return Buffer.from(await speech.arrayBuffer());\n  }\n\n  generateSilence(durationMs) {\n    // Generate silence buffer (simplified)\n    const sampleRate = 44100;\n    const samples = Math.floor(sampleRate * durationMs / 1000);\n    return Buffer.alloc(samples * 2); // 16-bit audio\n  }\n}\n\n// Usage\nconst audiobookGen = new AudiobookGenerator('condt_your_virtual_key');\n\nconst chapters = [\n  {\n    title: 'The Beginning',\n    content: 'It was the best of times, it was the worst of times...'\n  },\n  {\n    title: 'The Journey',\n    content: 'Our hero embarked on a quest that would change everything...'\n  }\n];\n\nconst audiobook = await audiobookGen.generateAudiobook(chapters);\nfs.writeFileSync('audiobook.wav', audiobook);\n"})}),"\n",(0,i.jsx)(n.h3,{id:"interactive-voice-response-ivr",children:"Interactive Voice Response (IVR)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"class IVRSystem {\n  constructor(apiKey) {\n    this.apiKey = apiKey;\n    this.voiceSettings = {\n      model: 'tts-1',\n      voice: 'alloy',\n      response_format: 'wav',\n      speed: 0.9  // Slightly slower for clarity\n    };\n  }\n\n  async generateIVRPrompts() {\n    const prompts = {\n      welcome: 'Welcome to our customer service. Please listen carefully as our menu options have changed.',\n      mainMenu: 'Press 1 for account information, Press 2 for technical support, Press 3 for billing questions, or Press 0 to speak with an operator.',\n      invalidOption: 'I\\'m sorry, that\\'s not a valid option. Please try again.',\n      transferring: 'Please hold while I transfer your call to the appropriate department.',\n      goodbye: 'Thank you for calling. Have a great day!'\n    };\n\n    const audioPrompts = {};\n    \n    for (const [key, text] of Object.entries(prompts)) {\n      console.log(`Generating ${key} prompt...`);\n      \n      const speech = await openai.audio.speech.create({\n        ...this.voiceSettings,\n        input: text\n      });\n      \n      audioPrompts[key] = Buffer.from(await speech.arrayBuffer());\n    }\n    \n    return audioPrompts;\n  }\n\n  async generateDynamicPrompt(template, variables) {\n    const text = template.replace(/\\{(\\w+)\\}/g, (match, key) => {\n      return variables[key] || match;\n    });\n    \n    const speech = await openai.audio.speech.create({\n      ...this.voiceSettings,\n      input: text\n    });\n    \n    return Buffer.from(await speech.arrayBuffer());\n  }\n}\n\n// Usage\nconst ivr = new IVRSystem('condt_your_virtual_key');\n\n// Generate static prompts\nconst prompts = await ivr.generateIVRPrompts();\n\n// Generate dynamic prompt\nconst dynamicPrompt = await ivr.generateDynamicPrompt(\n  'Hello {customerName}, your account balance is {balance} dollars.',\n  { customerName: 'John Smith', balance: '1,250.75' }\n);\n"})}),"\n",(0,i.jsx)(n.h3,{id:"podcastcontent-creation",children:"Podcast/Content Creation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"class PodcastGenerator {\n  constructor(apiKey) {\n    this.apiKey = apiKey;\n    this.hosts = {\n      host1: { voice: 'nova', name: 'Sarah' },\n      host2: { voice: 'onyx', name: 'Michael' }\n    };\n  }\n\n  async generatePodcastSegment(script) {\n    const audioSegments = [];\n    \n    for (const segment of script) {\n      const host = this.hosts[segment.speaker];\n      \n      if (!host) {\n        throw new Error(`Unknown speaker: ${segment.speaker}`);\n      }\n      \n      console.log(`${host.name}: ${segment.text}`);\n      \n      const speech = await openai.audio.speech.create({\n        model: 'tts-1-hd',\n        voice: host.voice,\n        input: segment.text,\n        response_format: 'mp3',\n        speed: segment.speed || 1.0\n      });\n      \n      const audioBuffer = Buffer.from(await speech.arrayBuffer());\n      audioSegments.push(audioBuffer);\n      \n      // Add natural pause between speakers\n      if (segment.pause) {\n        const pauseBuffer = this.generatePause(segment.pause);\n        audioSegments.push(pauseBuffer);\n      }\n    }\n    \n    return Buffer.concat(audioSegments);\n  }\n\n  generatePause(durationMs) {\n    // Generate brief pause/silence\n    const sampleRate = 44100;\n    const samples = Math.floor(sampleRate * durationMs / 1000);\n    return Buffer.alloc(samples * 2); // 16-bit silence\n  }\n}\n\n// Usage\nconst podcastGen = new PodcastGenerator('condt_your_virtual_key');\n\nconst script = [\n  {\n    speaker: 'host1',\n    text: 'Welcome back to Tech Talk Tuesday! I\\'m your host Sarah.',\n    pause: 500\n  },\n  {\n    speaker: 'host2', \n    text: 'And I\\'m Michael. Today we\\'re diving deep into artificial intelligence.',\n    pause: 1000\n  },\n  {\n    speaker: 'host1',\n    text: 'That\\'s right! We have some fascinating developments to discuss.',\n    speed: 1.1\n  }\n];\n\nconst podcastAudio = await podcastGen.generatePodcastSegment(script);\nfs.writeFileSync('podcast-segment.mp3', podcastAudio);\n"})}),"\n",(0,i.jsx)(n.h2,{id:"error-handling-and-optimization",children:"Error Handling and Optimization"}),"\n",(0,i.jsx)(n.h3,{id:"common-tts-errors",children:"Common TTS Errors"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"try {\n  const speech = await openai.audio.speech.create({\n    model: 'tts-1',\n    voice: 'alloy',\n    input: 'Hello world!'\n  });\n} catch (error) {\n  switch (error.code) {\n    case 'text_too_long':\n      console.log('Text exceeds maximum length (4096 characters)');\n      // Split text into chunks\n      break;\n    case 'invalid_voice':\n      console.log('Voice not available for selected model');\n      // Fallback to default voice\n      break;\n    case 'unsupported_language':\n      console.log('Language not supported by selected voice');\n      // Switch to appropriate voice\n      break;\n    case 'rate_limit_exceeded':\n      console.log('Too many requests, please wait');\n      // Implement retry with backoff\n      break;\n    case 'quota_exceeded':\n      console.log('Monthly quota exceeded');\n      // Switch to different provider or upgrade plan\n      break;\n    default:\n      console.log('TTS error:', error.message);\n  }\n}\n"})}),"\n",(0,i.jsx)(n.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"class OptimizedTTS {\n  constructor(apiKey) {\n    this.apiKey = apiKey;\n    this.cache = new Map();\n    this.requestQueue = [];\n    this.processing = false;\n  }\n\n  getCacheKey(text, options) {\n    return crypto.createHash('md5')\n      .update(JSON.stringify({ text, ...options }))\n      .digest('hex');\n  }\n\n  async synthesizeWithCache(text, options = {}) {\n    const cacheKey = this.getCacheKey(text, options);\n    \n    // Check cache first\n    if (this.cache.has(cacheKey)) {\n      console.log('Using cached audio');\n      return this.cache.get(cacheKey);\n    }\n    \n    // Generate new audio\n    const audio = await this.synthesize(text, options);\n    \n    // Cache the result\n    this.cache.set(cacheKey, audio);\n    \n    return audio;\n  }\n\n  async synthesize(text, options) {\n    return new Promise((resolve, reject) => {\n      this.requestQueue.push({ text, options, resolve, reject });\n      this.processQueue();\n    });\n  }\n\n  async processQueue() {\n    if (this.processing || this.requestQueue.length === 0) {\n      return;\n    }\n    \n    this.processing = true;\n    \n    while (this.requestQueue.length > 0) {\n      const { text, options, resolve, reject } = this.requestQueue.shift();\n      \n      try {\n        const speech = await openai.audio.speech.create({\n          model: 'tts-1',\n          input: text,\n          ...options\n        });\n        \n        const buffer = Buffer.from(await speech.arrayBuffer());\n        resolve(buffer);\n        \n        // Rate limiting - wait between requests\n        if (this.requestQueue.length > 0) {\n          await new Promise(r => setTimeout(r, 100));\n        }\n      } catch (error) {\n        reject(error);\n      }\n    }\n    \n    this.processing = false;\n  }\n}\n"})}),"\n",(0,i.jsx)(n.h2,{id:"integration-with-real-time-communication",children:"Integration with Real-Time Communication"}),"\n",(0,i.jsx)(n.h3,{id:"websocket-tts-for-real-time-applications",children:"WebSocket TTS for Real-Time Applications"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"class RealTimeTTSClient {\n  constructor(apiKey) {\n    this.apiKey = apiKey;\n    this.connection = null;\n    this.audioQueue = [];\n  }\n\n  async connect() {\n    this.connection = new WebSocket('wss://api.conduit.yourdomain.com/v1/audio/realtime-tts', {\n      headers: {\n        'Authorization': `Bearer ${this.apiKey}`\n      }\n    });\n\n    this.connection.on('open', () => {\n      console.log('Real-time TTS connected');\n      this.sendConfiguration();\n    });\n\n    this.connection.on('message', (data) => {\n      if (data instanceof Buffer) {\n        this.handleAudioData(data);\n      } else {\n        this.handleControlMessage(JSON.parse(data));\n      }\n    });\n  }\n\n  sendConfiguration() {\n    this.connection.send(JSON.stringify({\n      type: 'configure',\n      config: {\n        voice: 'nova',\n        format: 'pcm16',\n        sample_rate: 24000,\n        streaming: true\n      }\n    }));\n  }\n\n  speakText(text) {\n    if (this.connection && this.connection.readyState === WebSocket.OPEN) {\n      this.connection.send(JSON.stringify({\n        type: 'speak',\n        text: text,\n        immediate: true\n      }));\n    }\n  }\n\n  handleAudioData(audioData) {\n    // Stream audio directly to speakers\n    this.playAudioChunk(audioData);\n  }\n\n  playAudioChunk(audioData) {\n    // Implement real-time audio playback\n    // This would use Web Audio API in browser or audio libraries in Node.js\n    console.log(`Playing audio chunk: ${audioData.length} bytes`);\n  }\n}\n\n// Usage for real-time applications\nconst realtimeTTS = new RealTimeTTSClient('condt_your_virtual_key');\nawait realtimeTTS.connect();\n\n// Speak text immediately with minimal latency\nrealtimeTTS.speakText('This will be spoken immediately!');\nrealtimeTTS.speakText('And this will follow seamlessly.');\n"})}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Real-Time Audio"}),": Build conversational applications with ",(0,i.jsx)(n.a,{href:"real-time-audio",children:"real-time audio"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Speech-to-Text"}),": Combine with ",(0,i.jsx)(n.a,{href:"speech-to-text",children:"transcription services"})," for complete audio workflows"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Audio Providers"}),": Compare features across ",(0,i.jsx)(n.a,{href:"providers",children:"audio providers"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Integration Examples"}),": See complete ",(0,i.jsx)(n.a,{href:"../clients/overview",children:"client integration patterns"})]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>r});var s=t(6540);const i={},o=s.createContext(i);function a(e){const n=s.useContext(o);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);