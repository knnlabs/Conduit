"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[1260],{3181:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"features/audio-services","title":"Audio Services","description":"ConduitLLM provides comprehensive audio processing capabilities through a unified API that supports Speech-to-Text (transcription), Text-to-Speech (TTS), and real-time bidirectional audio streaming across multiple providers.","source":"@site/docs/features/audio-services.md","sourceDirName":"features","slug":"/features/audio-services","permalink":"/Conduit/docs/features/audio-services","draft":false,"unlisted":false,"editUrl":"https://github.com/knnlabs/Conduit/tree/main/website/docs/features/audio-services.md","tags":[],"version":"current","frontMatter":{}}');var r=n(4848),s=n(8453);const l={},o="Audio Services",c={},d=[{value:"Overview",id:"overview",level:2},{value:"Supported Providers",id:"supported-providers",level:2},{value:"Currently Available",id:"currently-available",level:3},{value:"Core Capabilities",id:"core-capabilities",level:2},{value:"Speech-to-Text (Transcription)",id:"speech-to-text-transcription",level:3},{value:"Text-to-Speech (TTS)",id:"text-to-speech-tts",level:3},{value:"Real-time Audio",id:"real-time-audio",level:3},{value:"Architecture",id:"architecture",level:2},{value:"Key Features",id:"key-features",level:2},{value:"1. Intelligent Routing",id:"1-intelligent-routing",level:3},{value:"2. Production Monitoring",id:"2-production-monitoring",level:3},{value:"3. Security &amp; Compliance",id:"3-security--compliance",level:3},{value:"4. Advanced Audio Processing",id:"4-advanced-audio-processing",level:3},{value:"Quick Start",id:"quick-start",level:2},{value:"Basic Transcription",id:"basic-transcription",level:3},{value:"Text-to-Speech",id:"text-to-speech",level:3},{value:"Real-time Conversation",id:"real-time-conversation",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Next Steps",id:"next-steps",level:2}];function a(e){const i={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(i.header,{children:(0,r.jsx)(i.h1,{id:"audio-services",children:"Audio Services"})}),"\n",(0,r.jsx)(i.p,{children:"ConduitLLM provides comprehensive audio processing capabilities through a unified API that supports Speech-to-Text (transcription), Text-to-Speech (TTS), and real-time bidirectional audio streaming across multiple providers."}),"\n",(0,r.jsx)(i.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(i.p,{children:"The audio services in ConduitLLM offer:"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Unified Interface"}),": Single API for all audio operations across providers"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Provider Flexibility"}),": Switch between providers without changing code"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Advanced Features"}),": Real-time streaming, voice cloning, and conversational AI"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Production Ready"}),": Built-in monitoring, health checks, and error handling"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Cost Optimization"}),": Automatic routing based on cost, quality, or latency"]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"supported-providers",children:"Supported Providers"}),"\n",(0,r.jsx)(i.h3,{id:"currently-available",children:"Currently Available"}),"\n",(0,r.jsxs)(i.table,{children:[(0,r.jsx)(i.thead,{children:(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.th,{children:"Provider"}),(0,r.jsx)(i.th,{children:"Transcription"}),(0,r.jsx)(i.th,{children:"Text-to-Speech"}),(0,r.jsx)(i.th,{children:"Real-time Audio"}),(0,r.jsx)(i.th,{children:"Notes"})]})}),(0,r.jsxs)(i.tbody,{children:[(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:(0,r.jsx)(i.strong,{children:"OpenAI"})}),(0,r.jsx)(i.td,{children:"\u2705 Whisper"}),(0,r.jsx)(i.td,{children:"\u2705 TTS"}),(0,r.jsx)(i.td,{children:"\u2705 GPT-4o Realtime"}),(0,r.jsx)(i.td,{children:"Industry standard, excellent quality"})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:(0,r.jsx)(i.strong,{children:"Azure OpenAI"})}),(0,r.jsx)(i.td,{children:"\u2705 Whisper"}),(0,r.jsx)(i.td,{children:"\u2705 TTS"}),(0,r.jsx)(i.td,{children:"\u2705"}),(0,r.jsx)(i.td,{children:"Microsoft-hosted, enterprise SLAs"})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:(0,r.jsx)(i.strong,{children:"Google Cloud"})}),(0,r.jsx)(i.td,{children:"\u2705 Speech-to-Text"}),(0,r.jsx)(i.td,{children:"\u2705 Text-to-Speech"}),(0,r.jsx)(i.td,{children:"\u274c"}),(0,r.jsx)(i.td,{children:"Wide language support"})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:(0,r.jsx)(i.strong,{children:"AWS"})}),(0,r.jsx)(i.td,{children:"\u2705 Transcribe"}),(0,r.jsx)(i.td,{children:"\u2705 Polly"}),(0,r.jsx)(i.td,{children:"\u274c"}),(0,r.jsx)(i.td,{children:"Cost-effective, good integration"})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:(0,r.jsx)(i.strong,{children:"ElevenLabs"})}),(0,r.jsx)(i.td,{children:"\u274c"}),(0,r.jsx)(i.td,{children:"\u2705 Premium TTS"}),(0,r.jsx)(i.td,{children:"\u2705 Conversational"}),(0,r.jsx)(i.td,{children:"Best-in-class voice quality"})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:(0,r.jsx)(i.strong,{children:"Ultravox"})}),(0,r.jsx)(i.td,{children:"\u274c"}),(0,r.jsx)(i.td,{children:"\u274c"}),(0,r.jsx)(i.td,{children:"\u2705"}),(0,r.jsx)(i.td,{children:"Ultra-low latency telephony"})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:(0,r.jsx)(i.strong,{children:"Groq"})}),(0,r.jsx)(i.td,{children:"\u2705 Whisper"}),(0,r.jsx)(i.td,{children:"\u274c"}),(0,r.jsx)(i.td,{children:"\u274c"}),(0,r.jsx)(i.td,{children:"Fastest transcription available"})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:(0,r.jsx)(i.strong,{children:"Deepgram"})}),(0,r.jsx)(i.td,{children:"\u2705"}),(0,r.jsx)(i.td,{children:"\u274c"}),(0,r.jsx)(i.td,{children:"\u2705"}),(0,r.jsx)(i.td,{children:"Real-time STT specialist"})]})]})]}),"\n",(0,r.jsx)(i.h2,{id:"core-capabilities",children:"Core Capabilities"}),"\n",(0,r.jsx)(i.h3,{id:"speech-to-text-transcription",children:"Speech-to-Text (Transcription)"}),"\n",(0,r.jsx)(i.p,{children:"Convert audio files or streams into text with support for:"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"50+ languages with automatic detection"}),"\n",(0,r.jsx)(i.li,{children:"Multiple output formats (text, JSON, SRT, VTT)"}),"\n",(0,r.jsx)(i.li,{children:"Word-level timestamps and confidence scores"}),"\n",(0,r.jsx)(i.li,{children:"Speaker diarization (provider-dependent)"}),"\n",(0,r.jsx)(i.li,{children:"Custom vocabulary and acoustic models"}),"\n"]}),"\n",(0,r.jsx)(i.h3,{id:"text-to-speech-tts",children:"Text-to-Speech (TTS)"}),"\n",(0,r.jsx)(i.p,{children:"Generate natural-sounding speech from text:"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Multiple voices per provider (100+ total)"}),"\n",(0,r.jsx)(i.li,{children:"Voice cloning capabilities (ElevenLabs)"}),"\n",(0,r.jsx)(i.li,{children:"Emotional and style control"}),"\n",(0,r.jsx)(i.li,{children:"SSML support for advanced control"}),"\n",(0,r.jsx)(i.li,{children:"Streaming for low-latency applications"}),"\n"]}),"\n",(0,r.jsx)(i.h3,{id:"real-time-audio",children:"Real-time Audio"}),"\n",(0,r.jsx)(i.p,{children:"Enable interactive voice conversations:"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Bidirectional audio streaming"}),"\n",(0,r.jsx)(i.li,{children:"Sub-100ms latency options"}),"\n",(0,r.jsx)(i.li,{children:"Voice activity detection (VAD)"}),"\n",(0,r.jsx)(i.li,{children:"Interruption handling"}),"\n",(0,r.jsx)(i.li,{children:"Function calling during conversations"}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"architecture",children:"Architecture"}),"\n",(0,r.jsx)(i.mermaid,{value:"graph TD\n    A[Client Application] --\x3e B[ConduitLLM Audio API]\n    B --\x3e C[Audio Router]\n    C --\x3e D[Provider Selection]\n    D --\x3e E[OpenAI]\n    D --\x3e F[Google Cloud]\n    D --\x3e G[AWS]\n    D --\x3e H[Other Providers]\n    \n    I[Health Checks] --\x3e C\n    J[Metrics Collector] --\x3e C\n    K[Cost Tracker] --\x3e C"}),"\n",(0,r.jsx)(i.h2,{id:"key-features",children:"Key Features"}),"\n",(0,r.jsx)(i.h3,{id:"1-intelligent-routing",children:"1. Intelligent Routing"}),"\n",(0,r.jsx)(i.p,{children:"The audio router automatically selects the best provider based on:"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Cost Optimization"}),": Choose the cheapest provider for the request"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Quality Priority"}),": Select providers with highest accuracy"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Latency Requirements"}),": Route to fastest responding provider"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Language Expertise"}),": Match provider strengths to language"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Availability"}),": Automatic failover when providers are down"]}),"\n"]}),"\n",(0,r.jsx)(i.h3,{id:"2-production-monitoring",children:"2. Production Monitoring"}),"\n",(0,r.jsx)(i.p,{children:"Built-in observability features:"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Health Checks"}),": Continuous provider availability monitoring"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Metrics Collection"}),": Prometheus-compatible metrics"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Distributed Tracing"}),": Correlation IDs across all requests"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Performance Tracking"}),": Latency and throughput monitoring"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Cost Attribution"}),": Per-request cost tracking"]}),"\n"]}),"\n",(0,r.jsx)(i.h3,{id:"3-security--compliance",children:"3. Security & Compliance"}),"\n",(0,r.jsx)(i.p,{children:"Enterprise-grade security features:"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Audio Encryption"}),": AES-256-GCM for data at rest and in transit"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"PII Detection"}),": Automatic detection and redaction"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Content Filtering"}),": Block inappropriate content"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Audit Logging"}),": Complete audit trail for compliance"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Access Control"}),": Virtual key-based permissions"]}),"\n"]}),"\n",(0,r.jsx)(i.h3,{id:"4-advanced-audio-processing",children:"4. Advanced Audio Processing"}),"\n",(0,r.jsx)(i.p,{children:"Additional capabilities:"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Format Conversion"}),": Automatic audio format handling"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Quality Enhancement"}),": Noise reduction and normalization"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Caching"}),": Intelligent caching for repeated requests"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Batch Processing"}),": Efficient handling of multiple files"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Webhook Support"}),": Async processing with callbacks"]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,r.jsx)(i.h3,{id:"basic-transcription",children:"Basic Transcription"}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{className:"language-csharp",children:'// Get a transcription client\nvar client = conduit.GetClient("whisper-1");\nif (client is IAudioTranscriptionClient transcriptionClient)\n{\n    var request = new AudioTranscriptionRequest\n    {\n        AudioData = File.ReadAllBytes("audio.mp3"),\n        Language = "en",\n        ResponseFormat = TranscriptionFormat.Json\n    };\n    \n    var response = await transcriptionClient.TranscribeAudioAsync(request);\n    Console.WriteLine($"Transcription: {response.Text}");\n}\n'})}),"\n",(0,r.jsx)(i.h3,{id:"text-to-speech",children:"Text-to-Speech"}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{className:"language-csharp",children:'// Generate speech from text\nvar client = conduit.GetClient("tts-1");\nif (client is ITextToSpeechClient ttsClient)\n{\n    var request = new TextToSpeechRequest\n    {\n        Input = "Hello, this is a test of the audio system.",\n        Voice = "nova",\n        ResponseFormat = AudioFormat.Mp3\n    };\n    \n    var response = await ttsClient.CreateSpeechAsync(request);\n    File.WriteAllBytes("output.mp3", response.AudioData);\n}\n'})}),"\n",(0,r.jsx)(i.h3,{id:"real-time-conversation",children:"Real-time Conversation"}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{className:"language-csharp",children:'// Create an interactive voice session\nvar client = conduit.GetClient("gpt-4o-realtime");\nif (client is IRealtimeAudioClient realtimeClient)\n{\n    var config = new RealtimeSessionConfig\n    {\n        Model = "gpt-4o-realtime-preview",\n        Voice = "alloy",\n        SystemPrompt = "You are a helpful assistant."\n    };\n    \n    var session = await realtimeClient.CreateSessionAsync(config);\n    // Stream audio in both directions...\n}\n'})}),"\n",(0,r.jsx)(i.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,r.jsxs)(i.ol,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Choose the Right Provider"}),": Match provider capabilities to your use case"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Handle Errors Gracefully"}),": Implement retry logic and fallback providers"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Optimize for Cost"}),": Use caching and batch processing where possible"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Monitor Performance"}),": Track latency and success rates"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Secure Your Audio"}),": Enable encryption for sensitive content"]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.a,{href:"/Conduit/docs/features/audio-providers",children:"Audio Provider Configuration"})," - Set up specific providers"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.a,{href:"/Conduit/docs/api-reference/audio",children:"Audio API Reference"})," - Detailed API documentation"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.a,{href:"/Conduit/docs/monitoring/metrics-monitoring",children:"Monitoring Guide"})," - Track audio service performance"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.a,{href:"/Conduit/docs/guides/budget-management",children:"Cost Management"})," - Control audio processing costs"]}),"\n"]})]})}function h(e={}){const{wrapper:i}={...(0,s.R)(),...e.components};return i?(0,r.jsx)(i,{...e,children:(0,r.jsx)(a,{...e})}):a(e)}},8453:(e,i,n)=>{n.d(i,{R:()=>l,x:()=>o});var t=n(6540);const r={},s=t.createContext(r);function l(e){const i=t.useContext(s);return t.useMemo((function(){return"function"==typeof e?e(i):{...i,...e}}),[i,e])}function o(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),t.createElement(s.Provider,{value:i},e.children)}}}]);