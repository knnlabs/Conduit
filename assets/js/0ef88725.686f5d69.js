"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[1800],{5990:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>p,frontMatter:()=>i,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"core-apis/chat-completions","title":"Chat Completions","description":"Comprehensive guide to chat completions API with multiple providers, streaming, and advanced features","source":"@site/docs/core-apis/chat-completions.md","sourceDirName":"core-apis","slug":"/core-apis/chat-completions","permalink":"/Conduit/docs/core-apis/chat-completions","draft":false,"unlisted":false,"editUrl":"https://github.com/knnlabs/Conduit/tree/main/website/docs/core-apis/chat-completions.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Chat Completions","description":"Comprehensive guide to chat completions API with multiple providers, streaming, and advanced features"}}');var o=t(4848),a=t(8453);const i={sidebar_position:2,title:"Chat Completions",description:"Comprehensive guide to chat completions API with multiple providers, streaming, and advanced features"},r="Chat Completions",c={},l=[{value:"Quick Start",id:"quick-start",level:2},{value:"Basic Chat Completion",id:"basic-chat-completion",level:3},{value:"Streaming Chat Completion",id:"streaming-chat-completion",level:3},{value:"Supported Models",id:"supported-models",level:2},{value:"OpenAI Models",id:"openai-models",level:3},{value:"Anthropic Models",id:"anthropic-models",level:3},{value:"Google Models",id:"google-models",level:3},{value:"Provider Selection",id:"provider-selection",level:3},{value:"Request Parameters",id:"request-parameters",level:2},{value:"Core Parameters",id:"core-parameters",level:3},{value:"Message Roles",id:"message-roles",level:3},{value:"Advanced Parameters",id:"advanced-parameters",level:3},{value:"Response Format",id:"response-format",level:2},{value:"Standard Response",id:"standard-response",level:3},{value:"Streaming Response",id:"streaming-response",level:3},{value:"Error Handling",id:"error-handling",level:3},{value:"Function Calling",id:"function-calling",level:2},{value:"Basic Function Calling",id:"basic-function-calling",level:3},{value:"Tools Format (Recommended)",id:"tools-format-recommended",level:3},{value:"Advanced Function Calling Patterns",id:"advanced-function-calling-patterns",level:3},{value:"Multimodal Support",id:"multimodal-support",level:2},{value:"Image Input",id:"image-input",level:3},{value:"Multiple Images",id:"multiple-images",level:3},{value:"Base64 Images",id:"base64-images",level:3},{value:"Advanced Use Cases",id:"advanced-use-cases",level:2},{value:"Conversation Management",id:"conversation-management",level:3},{value:"Content Generation Pipeline",id:"content-generation-pipeline",level:3},{value:"Code Review Assistant",id:"code-review-assistant",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Response Caching",id:"response-caching",level:3},{value:"Batch Processing",id:"batch-processing",level:3},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chat-completions",children:"Chat Completions"})}),"\n",(0,o.jsx)(n.p,{children:"The Chat Completions API is Conduit's primary endpoint for conversational AI, providing OpenAI-compatible text generation with support for multiple providers, streaming responses, function calling, and multimodal capabilities."}),"\n",(0,o.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,o.jsx)(n.h3,{id:"basic-chat-completion",children:"Basic Chat Completion"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-javascript",children:"import OpenAI from 'openai';\n\nconst openai = new OpenAI({\n  apiKey: 'condt_your_virtual_key',\n  baseURL: 'https://api.conduit.yourdomain.com/v1'\n});\n\nconst completion = await openai.chat.completions.create({\n  model: 'gpt-4',\n  messages: [\n    {\n      role: 'system',\n      content: 'You are a helpful assistant.'\n    },\n    {\n      role: 'user',\n      content: 'Explain quantum computing in simple terms.'\n    }\n  ],\n  max_tokens: 1000,\n  temperature: 0.7\n});\n\nconsole.log(completion.choices[0].message.content);\n"})}),"\n",(0,o.jsx)(n.h3,{id:"streaming-chat-completion",children:"Streaming Chat Completion"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-javascript",children:"const stream = await openai.chat.completions.create({\n  model: 'gpt-4',\n  messages: [\n    {\n      role: 'user',\n      content: 'Write a short story about a robot learning to paint.'\n    }\n  ],\n  stream: true,\n  max_tokens: 1500\n});\n\nfor await (const chunk of stream) {\n  const content = chunk.choices[0]?.delta?.content || '';\n  process.stdout.write(content);\n}\n"})}),"\n",(0,o.jsx)(n.h2,{id:"supported-models",children:"Supported Models"}),"\n",(0,o.jsx)(n.h3,{id:"openai-models",children:"OpenAI Models"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"Model"}),(0,o.jsx)(n.th,{children:"Context Length"}),(0,o.jsx)(n.th,{children:"Output Tokens"}),(0,o.jsx)(n.th,{children:"Cost (per 1K tokens)"}),(0,o.jsx)(n.th,{children:"Best For"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"gpt-4o"})}),(0,o.jsx)(n.td,{children:"128K"}),(0,o.jsx)(n.td,{children:"4K"}),(0,o.jsx)(n.td,{children:"$5.00 / $15.00"}),(0,o.jsx)(n.td,{children:"Latest multimodal"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"gpt-4-turbo"})}),(0,o.jsx)(n.td,{children:"128K"}),(0,o.jsx)(n.td,{children:"4K"}),(0,o.jsx)(n.td,{children:"$10.00 / $30.00"}),(0,o.jsx)(n.td,{children:"Complex reasoning"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"gpt-4"})}),(0,o.jsx)(n.td,{children:"8K"}),(0,o.jsx)(n.td,{children:"4K"}),(0,o.jsx)(n.td,{children:"$30.00 / $60.00"}),(0,o.jsx)(n.td,{children:"High-quality responses"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"gpt-3.5-turbo"})}),(0,o.jsx)(n.td,{children:"16K"}),(0,o.jsx)(n.td,{children:"4K"}),(0,o.jsx)(n.td,{children:"$0.50 / $1.50"}),(0,o.jsx)(n.td,{children:"General purpose"})]})]})]}),"\n",(0,o.jsx)(n.h3,{id:"anthropic-models",children:"Anthropic Models"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"Model"}),(0,o.jsx)(n.th,{children:"Context Length"}),(0,o.jsx)(n.th,{children:"Output Tokens"}),(0,o.jsx)(n.th,{children:"Cost (per 1K tokens)"}),(0,o.jsx)(n.th,{children:"Best For"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"claude-3-opus"})}),(0,o.jsx)(n.td,{children:"200K"}),(0,o.jsx)(n.td,{children:"4K"}),(0,o.jsx)(n.td,{children:"$15.00 / $75.00"}),(0,o.jsx)(n.td,{children:"Complex analysis"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"claude-3-sonnet"})}),(0,o.jsx)(n.td,{children:"200K"}),(0,o.jsx)(n.td,{children:"4K"}),(0,o.jsx)(n.td,{children:"$3.00 / $15.00"}),(0,o.jsx)(n.td,{children:"Balanced performance"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"claude-3-haiku"})}),(0,o.jsx)(n.td,{children:"200K"}),(0,o.jsx)(n.td,{children:"4K"}),(0,o.jsx)(n.td,{children:"$0.25 / $1.25"}),(0,o.jsx)(n.td,{children:"Fast responses"})]})]})]}),"\n",(0,o.jsx)(n.h3,{id:"google-models",children:"Google Models"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"Model"}),(0,o.jsx)(n.th,{children:"Context Length"}),(0,o.jsx)(n.th,{children:"Output Tokens"}),(0,o.jsx)(n.th,{children:"Cost (per 1K tokens)"}),(0,o.jsx)(n.th,{children:"Best For"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"gemini-pro"})}),(0,o.jsx)(n.td,{children:"32K"}),(0,o.jsx)(n.td,{children:"8K"}),(0,o.jsx)(n.td,{children:"$0.50 / $1.50"}),(0,o.jsx)(n.td,{children:"General purpose"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"gemini-ultra"})}),(0,o.jsx)(n.td,{children:"32K"}),(0,o.jsx)(n.td,{children:"8K"}),(0,o.jsx)(n.td,{children:"$TBD"}),(0,o.jsx)(n.td,{children:"Premium quality"})]})]})]}),"\n",(0,o.jsx)(n.h3,{id:"provider-selection",children:"Provider Selection"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-javascript",children:"// Automatic provider routing based on model\nconst gpt4Response = await openai.chat.completions.create({\n  model: 'gpt-4', // Routes to OpenAI\n  messages: [{ role: 'user', content: 'Hello' }]\n});\n\nconst claudeResponse = await openai.chat.completions.create({\n  model: 'claude-3-sonnet', // Routes to Anthropic\n  messages: [{ role: 'user', content: 'Hello' }]\n});\n\n// Provider preference via headers\nconst response = await openai.chat.completions.create({\n  model: 'gpt-4',\n  messages: [{ role: 'user', content: 'Hello' }],\n  headers: {\n    'X-Conduit-Provider-Preference': 'azure-openai,openai'\n  }\n});\n"})}),"\n",(0,o.jsx)(n.h2,{id:"request-parameters",children:"Request Parameters"}),"\n",(0,o.jsx)(n.h3,{id:"core-parameters",children:"Core Parameters"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-javascript",children:"const completion = await openai.chat.completions.create({\n  // Required parameters\n  model: 'gpt-4',\n  messages: [\n    { role: 'system', content: 'You are a helpful assistant.' },\n    { role: 'user', content: 'Hello!' }\n  ],\n  \n  // Generation parameters\n  max_tokens: 1000,        // Maximum response length\n  temperature: 0.7,        // Randomness (0.0-2.0)\n  top_p: 1.0,             // Nucleus sampling\n  frequency_penalty: 0.0,  // Reduce repetition (-2.0 to 2.0)\n  presence_penalty: 0.0,   // Encourage new topics (-2.0 to 2.0)\n  \n  // Response format\n  stream: false,           // Enable streaming\n  n: 1,                   // Number of completions\n  stop: null,             // Stop sequences\n  \n  // Advanced features\n  functions: [],          // Function definitions\n  function_call: 'auto',  // Function calling mode\n  tools: [],              // Tool definitions (newer format)\n  tool_choice: 'auto',    // Tool selection strategy\n  \n  // Metadata\n  user: 'user-123'        // User identifier\n});\n"})}),"\n",(0,o.jsx)(n.h3,{id:"message-roles",children:"Message Roles"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-javascript",children:"const messages = [\n  {\n    role: 'system',\n    content: 'You are a professional writing assistant. Help users improve their writing with constructive feedback.'\n  },\n  {\n    role: 'user',\n    content: 'Can you help me improve this sentence: \"The thing was good.\"'\n  },\n  {\n    role: 'assistant',\n    content: 'I\\'d be happy to help! That sentence could be more specific and descriptive. Here are some improvements:\\n\\n1. Replace \"thing\" with a specific noun\\n2. Use a more descriptive adjective than \"good\"\\n3. Add details about why it was good\\n\\nFor example: \"The presentation was engaging and well-structured.\"'\n  },\n  {\n    role: 'user',\n    content: 'Great! Now help me with this paragraph...'\n  }\n];\n"})}),"\n",(0,o.jsx)(n.h3,{id:"advanced-parameters",children:"Advanced Parameters"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-javascript",children:"// Fine-tuned generation control\nconst completion = await openai.chat.completions.create({\n  model: 'gpt-4',\n  messages: messages,\n  \n  // Temperature control for different use cases\n  temperature: 0.0,  // Deterministic (good for factual responses)\n  // temperature: 0.7,  // Balanced (good for general conversation)  \n  // temperature: 1.2,  // Creative (good for storytelling)\n  \n  // Token probability control\n  top_p: 0.9,        // Use top 90% probability mass\n  \n  // Repetition control\n  frequency_penalty: 0.5,  // Reduce word repetition\n  presence_penalty: 0.3,   // Encourage topic diversity\n  \n  // Response control\n  max_tokens: 2000,\n  stop: ['\\n\\nUser:', '\\n\\nAssistant:'], // Stop at conversation markers\n  \n  // Multiple responses\n  n: 3,              // Generate 3 different responses\n  \n  // Logging and debugging\n  logit_bias: {      // Bias certain tokens\n    50256: -100      // Strongly avoid end-of-text token\n  },\n  user: 'user-session-123'\n});\n"})}),"\n",(0,o.jsx)(n.h2,{id:"response-format",children:"Response Format"}),"\n",(0,o.jsx)(n.h3,{id:"standard-response",children:"Standard Response"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-json",children:'{\n  "id": "chatcmpl-abc123",\n  "object": "chat.completion",\n  "created": 1677652288,\n  "model": "gpt-4",\n  "choices": [\n    {\n      "index": 0,\n      "message": {\n        "role": "assistant",\n        "content": "I\'d be happy to explain quantum computing! Think of it as a fundamentally different way of processing information..."\n      },\n      "finish_reason": "stop"\n    }\n  ],\n  "usage": {\n    "prompt_tokens": 42,\n    "completion_tokens": 158,\n    "total_tokens": 200\n  },\n  "conduit_metadata": {\n    "provider": "openai",\n    "model_version": "gpt-4-0613",\n    "virtual_key_id": "550e8400-e29b-41d4-a716-446655440000",\n    "request_id": "req-abc123",\n    "processing_time_ms": 2341,\n    "cost": 0.012\n  }\n}\n'})}),"\n",(0,o.jsx)(n.h3,{id:"streaming-response",children:"Streaming Response"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-javascript",children:"// Streaming response handling\nconst stream = await openai.chat.completions.create({\n  model: 'gpt-4',\n  messages: [{ role: 'user', content: 'Tell me a story' }],\n  stream: true\n});\n\nlet fullContent = '';\n\nfor await (const chunk of stream) {\n  const delta = chunk.choices[0]?.delta;\n  \n  if (delta?.role) {\n    console.log(`\\n${delta.role}:`);\n  }\n  \n  if (delta?.content) {\n    process.stdout.write(delta.content);\n    fullContent += delta.content;\n  }\n  \n  if (chunk.choices[0]?.finish_reason) {\n    console.log(`\\n\\nFinished: ${chunk.choices[0].finish_reason}`);\n    break;\n  }\n}\n\nconsole.log('\\nFull response:', fullContent);\n"})}),"\n",(0,o.jsx)(n.h3,{id:"error-handling",children:"Error Handling"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-javascript",children:"try {\n  const completion = await openai.chat.completions.create({\n    model: 'gpt-4',\n    messages: [{ role: 'user', content: 'Hello' }]\n  });\n} catch (error) {\n  switch (error.code) {\n    case 'model_not_found':\n      console.log('Model not available or not supported');\n      break;\n    case 'context_length_exceeded':\n      console.log('Messages too long for model context window');\n      break;\n    case 'content_filter':\n      console.log('Content filtered by safety policies');\n      break;\n    case 'rate_limit_exceeded':\n      console.log('Rate limit hit, please wait');\n      break;\n    case 'insufficient_quota':\n      console.log('Quota exceeded for this model');\n      break;\n    default:\n      console.log('Chat completion error:', error.message);\n  }\n}\n"})}),"\n",(0,o.jsx)(n.h2,{id:"function-calling",children:"Function Calling"}),"\n",(0,o.jsx)(n.h3,{id:"basic-function-calling",children:"Basic Function Calling"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-javascript",children:"const completion = await openai.chat.completions.create({\n  model: 'gpt-4',\n  messages: [\n    {\n      role: 'user',\n      content: 'What is the weather like in Boston today?'\n    }\n  ],\n  functions: [\n    {\n      name: 'get_current_weather',\n      description: 'Get the current weather in a given location',\n      parameters: {\n        type: 'object',\n        properties: {\n          location: {\n            type: 'string',\n            description: 'The city and state, e.g. San Francisco, CA'\n          },\n          unit: {\n            type: 'string',\n            enum: ['celsius', 'fahrenheit'],\n            description: 'The temperature unit'\n          }\n        },\n        required: ['location']\n      }\n    }\n  ],\n  function_call: 'auto'\n});\n\n// Check if the model wants to call a function\nconst message = completion.choices[0].message;\n\nif (message.function_call) {\n  const functionName = message.function_call.name;\n  const functionArgs = JSON.parse(message.function_call.arguments);\n  \n  console.log(`Calling function: ${functionName}`);\n  console.log('Arguments:', functionArgs);\n  \n  // Call your function\n  const functionResult = await getWeather(functionArgs.location, functionArgs.unit);\n  \n  // Send the result back to the model\n  const followUp = await openai.chat.completions.create({\n    model: 'gpt-4',\n    messages: [\n      ...messages,\n      message,\n      {\n        role: 'function',\n        name: functionName,\n        content: JSON.stringify(functionResult)\n      }\n    ]\n  });\n  \n  console.log(followUp.choices[0].message.content);\n}\n"})}),"\n",(0,o.jsx)(n.h3,{id:"tools-format-recommended",children:"Tools Format (Recommended)"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-javascript",children:"// New tools format - more flexible than functions\nconst completion = await openai.chat.completions.create({\n  model: 'gpt-4',\n  messages: [\n    {\n      role: 'user',\n      content: 'I need to schedule a meeting for next Tuesday at 2 PM with John Smith to discuss the project proposal.'\n    }\n  ],\n  tools: [\n    {\n      type: 'function',\n      function: {\n        name: 'schedule_meeting',\n        description: 'Schedule a meeting with specified participants',\n        parameters: {\n          type: 'object',\n          properties: {\n            title: {\n              type: 'string',\n              description: 'Meeting title'\n            },\n            participants: {\n              type: 'array',\n              items: { type: 'string' },\n              description: 'List of participant names or emails'\n            },\n            date: {\n              type: 'string',\n              description: 'Meeting date in YYYY-MM-DD format'\n            },\n            time: {\n              type: 'string',\n              description: 'Meeting time in HH:MM format'\n            },\n            duration: {\n              type: 'number',\n              description: 'Meeting duration in minutes'\n            }\n          },\n          required: ['title', 'participants', 'date', 'time']\n        }\n      }\n    },\n    {\n      type: 'function',\n      function: {\n        name: 'check_calendar_availability',\n        description: 'Check if participants are available at specified time',\n        parameters: {\n          type: 'object',\n          properties: {\n            participants: {\n              type: 'array',\n              items: { type: 'string' }\n            },\n            date: { type: 'string' },\n            time: { type: 'string' },\n            duration: { type: 'number' }\n          },\n          required: ['participants', 'date', 'time']\n        }\n      }\n    }\n  ],\n  tool_choice: 'auto'\n});\n\n// Handle tool calls\nconst message = completion.choices[0].message;\n\nif (message.tool_calls) {\n  const toolResults = [];\n  \n  for (const toolCall of message.tool_calls) {\n    const functionName = toolCall.function.name;\n    const functionArgs = JSON.parse(toolCall.function.arguments);\n    \n    let result;\n    switch (functionName) {\n      case 'schedule_meeting':\n        result = await scheduleMeeting(functionArgs);\n        break;\n      case 'check_calendar_availability':\n        result = await checkAvailability(functionArgs);\n        break;\n      default:\n        result = { error: 'Unknown function' };\n    }\n    \n    toolResults.push({\n      tool_call_id: toolCall.id,\n      role: 'tool',\n      content: JSON.stringify(result)\n    });\n  }\n  \n  // Send results back to model\n  const followUp = await openai.chat.completions.create({\n    model: 'gpt-4',\n    messages: [\n      ...messages,\n      message,\n      ...toolResults\n    ]\n  });\n  \n  console.log(followUp.choices[0].message.content);\n}\n"})}),"\n",(0,o.jsx)(n.h3,{id:"advanced-function-calling-patterns",children:"Advanced Function Calling Patterns"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-javascript",children:"class FunctionCallingAssistant {\n  constructor(apiKey) {\n    this.openai = new OpenAI({\n      apiKey: apiKey,\n      baseURL: 'https://api.conduit.yourdomain.com/v1'\n    });\n    \n    this.tools = this.setupTools();\n    this.conversationHistory = [];\n  }\n\n  setupTools() {\n    return [\n      {\n        type: 'function',\n        function: {\n          name: 'web_search',\n          description: 'Search the web for current information',\n          parameters: {\n            type: 'object',\n            properties: {\n              query: { type: 'string', description: 'Search query' },\n              num_results: { type: 'number', description: 'Number of results to return', default: 5 }\n            },\n            required: ['query']\n          }\n        }\n      },\n      {\n        type: 'function',\n        function: {\n          name: 'calculate',\n          description: 'Perform mathematical calculations',\n          parameters: {\n            type: 'object',\n            properties: {\n              expression: { type: 'string', description: 'Mathematical expression to evaluate' }\n            },\n            required: ['expression']\n          }\n        }\n      },\n      {\n        type: 'function',\n        function: {\n          name: 'generate_code',\n          description: 'Generate code in specified programming language',\n          parameters: {\n            type: 'object',\n            properties: {\n              language: { type: 'string', description: 'Programming language' },\n              description: { type: 'string', description: 'What the code should do' },\n              include_comments: { type: 'boolean', description: 'Include explanatory comments', default: true }\n            },\n            required: ['language', 'description']\n          }\n        }\n      }\n    ];\n  }\n\n  async chat(userMessage) {\n    // Add user message to history\n    this.conversationHistory.push({\n      role: 'user',\n      content: userMessage\n    });\n\n    let response = await this.openai.chat.completions.create({\n      model: 'gpt-4',\n      messages: [\n        {\n          role: 'system',\n          content: 'You are a helpful assistant with access to web search, calculation, and code generation tools. Use these tools when needed to provide accurate and helpful responses.'\n        },\n        ...this.conversationHistory\n      ],\n      tools: this.tools,\n      tool_choice: 'auto'\n    });\n\n    let message = response.choices[0].message;\n\n    // Handle multiple rounds of tool calls\n    while (message.tool_calls) {\n      // Add assistant message with tool calls to history\n      this.conversationHistory.push(message);\n\n      // Process all tool calls\n      const toolResults = await this.procesToolCalls(message.tool_calls);\n      \n      // Add tool results to history\n      this.conversationHistory.push(...toolResults);\n\n      // Get next response from assistant\n      response = await this.openai.chat.completions.create({\n        model: 'gpt-4',\n        messages: [\n          {\n            role: 'system',\n            content: 'You are a helpful assistant with access to web search, calculation, and code generation tools.'\n          },\n          ...this.conversationHistory\n        ],\n        tools: this.tools,\n        tool_choice: 'auto'\n      });\n\n      message = response.choices[0].message;\n    }\n\n    // Add final assistant message to history\n    this.conversationHistory.push(message);\n\n    return message.content;\n  }\n\n  async procesToolCalls(toolCalls) {\n    const results = [];\n\n    for (const toolCall of toolCalls) {\n      const functionName = toolCall.function.name;\n      const functionArgs = JSON.parse(toolCall.function.arguments);\n\n      console.log(`Calling ${functionName} with:`, functionArgs);\n\n      let result;\n      try {\n        switch (functionName) {\n          case 'web_search':\n            result = await this.webSearch(functionArgs.query, functionArgs.num_results);\n            break;\n          case 'calculate':\n            result = await this.calculate(functionArgs.expression);\n            break;\n          case 'generate_code':\n            result = await this.generateCode(functionArgs.language, functionArgs.description, functionArgs.include_comments);\n            break;\n          default:\n            result = { error: `Unknown function: ${functionName}` };\n        }\n      } catch (error) {\n        result = { error: error.message };\n      }\n\n      results.push({\n        tool_call_id: toolCall.id,\n        role: 'tool',\n        content: JSON.stringify(result)\n      });\n    }\n\n    return results;\n  }\n\n  async webSearch(query, numResults = 5) {\n    // Implement web search (replace with actual search API)\n    try {\n      const response = await fetch(`/api/search?q=${encodeURIComponent(query)}&limit=${numResults}`);\n      const results = await response.json();\n      \n      return {\n        query: query,\n        results: results.slice(0, numResults),\n        timestamp: new Date().toISOString()\n      };\n    } catch (error) {\n      return { error: 'Search service unavailable' };\n    }\n  }\n\n  async calculate(expression) {\n    try {\n      // Safe expression evaluation (implement proper math parser)\n      const result = eval(expression); // WARNING: Use proper math parser in production\n      \n      return {\n        expression: expression,\n        result: result,\n        type: typeof result\n      };\n    } catch (error) {\n      return { error: 'Invalid mathematical expression' };\n    }\n  }\n\n  async generateCode(language, description, includeComments = true) {\n    // Use another AI model or code generation service\n    const codeCompletion = await this.openai.chat.completions.create({\n      model: 'gpt-4',\n      messages: [\n        {\n          role: 'system',\n          content: `You are an expert ${language} programmer. Generate clean, efficient code based on the description. ${includeComments ? 'Include helpful comments.' : 'Minimize comments.'}`\n        },\n        {\n          role: 'user',\n          content: `Generate ${language} code that ${description}`\n        }\n      ],\n      temperature: 0.2 // Lower temperature for more consistent code\n    });\n\n    const code = codeCompletion.choices[0].message.content;\n\n    return {\n      language: language,\n      description: description,\n      code: code,\n      includes_comments: includeComments\n    };\n  }\n}\n\n// Usage\nconst assistant = new FunctionCallingAssistant('condt_your_virtual_key');\n\nconst response = await assistant.chat('What is the current stock price of Apple and calculate what 100 shares would cost?');\nconsole.log(response);\n"})}),"\n",(0,o.jsx)(n.h2,{id:"multimodal-support",children:"Multimodal Support"}),"\n",(0,o.jsx)(n.h3,{id:"image-input",children:"Image Input"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-javascript",children:"// Send image with text for analysis\nconst completion = await openai.chat.completions.create({\n  model: 'gpt-4o', // Vision-enabled model\n  messages: [\n    {\n      role: 'user',\n      content: [\n        {\n          type: 'text',\n          text: 'What do you see in this image? Describe it in detail.'\n        },\n        {\n          type: 'image_url',\n          image_url: {\n            url: 'https://example.com/image.jpg',\n            detail: 'high' // 'low', 'high', or 'auto'\n          }\n        }\n      ]\n    }\n  ],\n  max_tokens: 1000\n});\n\nconsole.log(completion.choices[0].message.content);\n"})}),"\n",(0,o.jsx)(n.h3,{id:"multiple-images",children:"Multiple Images"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-javascript",children:"const completion = await openai.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [\n    {\n      role: 'user',\n      content: [\n        {\n          type: 'text',\n          text: 'Compare these two images and tell me the differences.'\n        },\n        {\n          type: 'image_url',\n          image_url: {\n            url: 'https://example.com/image1.jpg',\n            detail: 'high'\n          }\n        },\n        {\n          type: 'image_url',\n          image_url: {\n            url: 'https://example.com/image2.jpg',\n            detail: 'high'\n          }\n        }\n      ]\n    }\n  ]\n});\n"})}),"\n",(0,o.jsx)(n.h3,{id:"base64-images",children:"Base64 Images"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-javascript",children:"import fs from 'fs';\n\n// Read and encode image\nconst imageBuffer = fs.readFileSync('path/to/image.jpg');\nconst base64Image = imageBuffer.toString('base64');\n\nconst completion = await openai.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [\n    {\n      role: 'user',\n      content: [\n        {\n          type: 'text',\n          text: 'Analyze this image and extract any text you see.'\n        },\n        {\n          type: 'image_url',\n          image_url: {\n            url: `data:image/jpeg;base64,${base64Image}`,\n            detail: 'high'\n          }\n        }\n      ]\n    }\n  ]\n});\n"})}),"\n",(0,o.jsx)(n.h2,{id:"advanced-use-cases",children:"Advanced Use Cases"}),"\n",(0,o.jsx)(n.h3,{id:"conversation-management",children:"Conversation Management"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-javascript",children:"class ConversationManager {\n  constructor(apiKey, systemPrompt) {\n    this.openai = new OpenAI({\n      apiKey: apiKey,\n      baseURL: 'https://api.conduit.yourdomain.com/v1'\n    });\n    \n    this.messages = [\n      { role: 'system', content: systemPrompt }\n    ];\n    \n    this.maxMessages = 20; // Keep conversation manageable\n  }\n\n  async sendMessage(userMessage, options = {}) {\n    // Add user message\n    this.messages.push({\n      role: 'user',\n      content: userMessage\n    });\n\n    // Manage conversation length\n    this.trimConversation();\n\n    const completion = await this.openai.chat.completions.create({\n      model: 'gpt-4',\n      messages: this.messages,\n      temperature: 0.7,\n      max_tokens: 1000,\n      ...options\n    });\n\n    const assistantMessage = completion.choices[0].message;\n    \n    // Add assistant response to conversation\n    this.messages.push(assistantMessage);\n\n    return {\n      content: assistantMessage.content,\n      usage: completion.usage,\n      metadata: completion.conduit_metadata\n    };\n  }\n\n  trimConversation() {\n    // Keep system message and last N messages\n    if (this.messages.length > this.maxMessages) {\n      const systemMessage = this.messages[0];\n      const recentMessages = this.messages.slice(-this.maxMessages + 1);\n      this.messages = [systemMessage, ...recentMessages];\n    }\n  }\n\n  getConversationSummary() {\n    return this.messages\n      .filter(msg => msg.role !== 'system')\n      .map(msg => `${msg.role}: ${msg.content}`)\n      .join('\\n\\n');\n  }\n\n  clearConversation() {\n    const systemMessage = this.messages[0];\n    this.messages = [systemMessage];\n  }\n\n  exportConversation() {\n    return {\n      messages: this.messages,\n      timestamp: new Date().toISOString(),\n      messageCount: this.messages.length - 1 // Exclude system message\n    };\n  }\n}\n\n// Usage\nconst conversation = new ConversationManager(\n  'condt_your_virtual_key',\n  'You are a helpful coding assistant. Provide clear, practical advice and code examples.'\n);\n\nconst response1 = await conversation.sendMessage('How do I sort an array in JavaScript?');\nconsole.log(response1.content);\n\nconst response2 = await conversation.sendMessage('Can you show me a more complex example with custom sorting?');\nconsole.log(response2.content);\n"})}),"\n",(0,o.jsx)(n.h3,{id:"content-generation-pipeline",children:"Content Generation Pipeline"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-javascript",children:"class ContentGenerator {\n  constructor(apiKey) {\n    this.openai = new OpenAI({\n      apiKey: apiKey,\n      baseURL: 'https://api.conduit.yourdomain.com/v1'\n    });\n  }\n\n  async generateBlogPost(topic, targetAudience, wordCount = 1000) {\n    // Step 1: Generate outline\n    const outline = await this.generateOutline(topic, targetAudience);\n    \n    // Step 2: Generate introduction\n    const introduction = await this.generateIntroduction(topic, outline, targetAudience);\n    \n    // Step 3: Generate main content sections\n    const sections = await this.generateSections(outline, targetAudience, wordCount);\n    \n    // Step 4: Generate conclusion\n    const conclusion = await this.generateConclusion(topic, sections);\n    \n    // Step 5: Generate SEO metadata\n    const seoMetadata = await this.generateSEOMetadata(topic, introduction);\n\n    return {\n      title: outline.title,\n      introduction: introduction,\n      sections: sections,\n      conclusion: conclusion,\n      seo: seoMetadata,\n      wordCount: this.calculateWordCount([introduction, ...sections, conclusion]),\n      generatedAt: new Date().toISOString()\n    };\n  }\n\n  async generateOutline(topic, audience) {\n    const completion = await this.openai.chat.completions.create({\n      model: 'gpt-4',\n      messages: [\n        {\n          role: 'system',\n          content: 'You are an expert content strategist. Create detailed blog post outlines that are engaging and well-structured.'\n        },\n        {\n          role: 'user',\n          content: `Create a detailed outline for a blog post about \"${topic}\" targeting ${audience}. Include a compelling title and 4-6 main sections with brief descriptions.`\n        }\n      ],\n      temperature: 0.8,\n      max_tokens: 800\n    });\n\n    // Parse the outline (implement proper parsing logic)\n    const outlineText = completion.choices[0].message.content;\n    return this.parseOutline(outlineText);\n  }\n\n  async generateSections(outline, audience, targetWordCount) {\n    const sections = [];\n    const wordsPerSection = Math.floor(targetWordCount * 0.7 / outline.sections.length);\n\n    for (const section of outline.sections) {\n      const completion = await this.openai.chat.completions.create({\n        model: 'gpt-4',\n        messages: [\n          {\n            role: 'system',\n            content: `You are a skilled writer creating content for ${audience}. Write engaging, informative content with a conversational tone.`\n          },\n          {\n            role: 'user',\n            content: `Write a ${wordsPerSection}-word section about \"${section.title}\". Context: ${section.description}`\n          }\n        ],\n        temperature: 0.7,\n        max_tokens: Math.floor(wordsPerSection * 1.5) // Allow some buffer\n      });\n\n      sections.push({\n        title: section.title,\n        content: completion.choices[0].message.content,\n        wordCount: this.countWords(completion.choices[0].message.content)\n      });\n\n      // Small delay to respect rate limits\n      await new Promise(resolve => setTimeout(resolve, 100));\n    }\n\n    return sections;\n  }\n\n  async generateSEOMetadata(topic, introduction) {\n    const completion = await this.openai.chat.completions.create({\n      model: 'gpt-3.5-turbo', // Faster/cheaper for SEO metadata\n      messages: [\n        {\n          role: 'system',\n          content: 'You are an SEO expert. Generate compelling meta descriptions, titles, and keywords.'\n        },\n        {\n          role: 'user',\n          content: `Generate SEO metadata for a blog post about \"${topic}\". Introduction: \"${introduction.substring(0, 500)}...\"`\n        }\n      ],\n      temperature: 0.3,\n      max_tokens: 300\n    });\n\n    return this.parseSEOMetadata(completion.choices[0].message.content);\n  }\n\n  parseOutline(outlineText) {\n    // Implement proper outline parsing\n    const lines = outlineText.split('\\n').filter(line => line.trim());\n    const title = lines[0].replace(/^#*\\s*/, '');\n    \n    const sections = lines\n      .filter(line => line.includes('##') || line.includes('-'))\n      .map(line => ({\n        title: line.replace(/^#*-*\\s*/, ''),\n        description: `Content about ${line.replace(/^#*-*\\s*/, '')}`\n      }));\n\n    return { title, sections };\n  }\n\n  countWords(text) {\n    return text.split(/\\s+/).filter(word => word.length > 0).length;\n  }\n\n  calculateWordCount(sections) {\n    return sections.reduce((total, section) => {\n      return total + (typeof section === 'string' ? \n        this.countWords(section) : \n        this.countWords(section.content || section));\n    }, 0);\n  }\n}\n\n// Usage\nconst generator = new ContentGenerator('condt_your_virtual_key');\n\nconst blogPost = await generator.generateBlogPost(\n  'The Future of Artificial Intelligence in Healthcare',\n  'healthcare professionals and technology enthusiasts',\n  1500\n);\n\nconsole.log('Generated blog post:', blogPost.title);\nconsole.log('Word count:', blogPost.wordCount);\n"})}),"\n",(0,o.jsx)(n.h3,{id:"code-review-assistant",children:"Code Review Assistant"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-javascript",children:"class CodeReviewAssistant {\n  constructor(apiKey) {\n    this.openai = new OpenAI({\n      apiKey: apiKey,\n      baseURL: 'https://api.conduit.yourdomain.com/v1'\n    });\n  }\n\n  async reviewCode(code, language, context = '') {\n    const completion = await this.openai.chat.completions.create({\n      model: 'gpt-4',\n      messages: [\n        {\n          role: 'system',\n          content: `You are an expert ${language} developer and code reviewer. Provide detailed, constructive feedback on code quality, security, performance, and best practices. Format your response with clear sections for different types of feedback.`\n        },\n        {\n          role: 'user',\n          content: `Please review this ${language} code${context ? ` (Context: ${context})` : ''}:\\n\\n\\`\\`\\`${language}\\n${code}\\n\\`\\`\\``\n        }\n      ],\n      temperature: 0.3,\n      max_tokens: 2000\n    });\n\n    return this.parseCodeReview(completion.choices[0].message.content);\n  }\n\n  async suggestImprovements(code, language, focus = 'general') {\n    const focusPrompts = {\n      'performance': 'Focus on performance optimizations and efficiency improvements.',\n      'security': 'Focus on security vulnerabilities and best practices.',\n      'readability': 'Focus on code readability, maintainability, and documentation.',\n      'testing': 'Focus on testability and suggest unit tests.',\n      'general': 'Provide comprehensive feedback on all aspects.'\n    };\n\n    const completion = await this.openai.chat.completions.create({\n      model: 'gpt-4',\n      messages: [\n        {\n          role: 'system',\n          content: `You are an expert ${language} developer. ${focusPrompts[focus]} Provide specific, actionable suggestions with improved code examples.`\n        },\n        {\n          role: 'user',\n          content: `Suggest improvements for this ${language} code:\\n\\n\\`\\`\\`${language}\\n${code}\\n\\`\\`\\``\n        }\n      ],\n      temperature: 0.4,\n      max_tokens: 2500\n    });\n\n    return completion.choices[0].message.content;\n  }\n\n  async explainCode(code, language, audienceLevel = 'intermediate') {\n    const completion = await this.openai.chat.completions.create({\n      model: 'gpt-4',\n      messages: [\n        {\n          role: 'system',\n          content: `You are a patient programming instructor. Explain code clearly for ${audienceLevel} level programmers. Break down complex concepts and explain the reasoning behind design decisions.`\n        },\n        {\n          role: 'user',\n          content: `Please explain what this ${language} code does and how it works:\\n\\n\\`\\`\\`${language}\\n${code}\\n\\`\\`\\``\n        }\n      ],\n      temperature: 0.5,\n      max_tokens: 1500\n    });\n\n    return completion.choices[0].message.content;\n  }\n\n  parseCodeReview(reviewText) {\n    // Parse structured review (implement proper parsing)\n    const sections = {\n      summary: '',\n      strengths: [],\n      issues: [],\n      suggestions: [],\n      rating: null\n    };\n\n    // Simple parsing logic (enhance as needed)\n    const lines = reviewText.split('\\n');\n    let currentSection = 'summary';\n    \n    for (const line of lines) {\n      const lowerLine = line.toLowerCase().trim();\n      \n      if (lowerLine.includes('strengths') || lowerLine.includes('good')) {\n        currentSection = 'strengths';\n      } else if (lowerLine.includes('issues') || lowerLine.includes('problems')) {\n        currentSection = 'issues';\n      } else if (lowerLine.includes('suggestions') || lowerLine.includes('improvements')) {\n        currentSection = 'suggestions';\n      } else if (line.trim().startsWith('-') || line.trim().startsWith('*')) {\n        const item = line.replace(/^[-*]\\s*/, '').trim();\n        if (item && sections[currentSection] instanceof Array) {\n          sections[currentSection].push(item);\n        }\n      } else if (currentSection === 'summary' && line.trim()) {\n        sections.summary += line + '\\n';\n      }\n    }\n\n    return sections;\n  }\n}\n\n// Usage\nconst reviewer = new CodeReviewAssistant('condt_your_virtual_key');\n\nconst code = `\nfunction calculateTotal(items) {\n  var total = 0;\n  for (var i = 0; i < items.length; i++) {\n    total += items[i].price * items[i].quantity;\n  }\n  return total;\n}\n`;\n\nconst review = await reviewer.reviewCode(code, 'javascript', 'e-commerce cart calculation');\nconsole.log('Code review:', review);\n\nconst improvements = await reviewer.suggestImprovements(code, 'javascript', 'performance');\nconsole.log('Suggested improvements:', improvements);\n"})}),"\n",(0,o.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,o.jsx)(n.h3,{id:"response-caching",children:"Response Caching"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-javascript",children:"class CachedChatClient {\n  constructor(apiKey, cacheOptions = {}) {\n    this.openai = new OpenAI({\n      apiKey: apiKey,\n      baseURL: 'https://api.conduit.yourdomain.com/v1'\n    });\n    \n    this.cache = new Map();\n    this.maxCacheSize = cacheOptions.maxSize || 1000;\n    this.cacheTimeout = cacheOptions.timeout || 3600000; // 1 hour\n  }\n\n  getCacheKey(messages, options) {\n    const key = {\n      messages: messages.map(m => ({ role: m.role, content: m.content })),\n      model: options.model,\n      temperature: options.temperature,\n      max_tokens: options.max_tokens\n    };\n    \n    return JSON.stringify(key);\n  }\n\n  async chat(messages, options = {}) {\n    const cacheKey = this.getCacheKey(messages, options);\n    \n    // Check cache\n    const cached = this.cache.get(cacheKey);\n    if (cached && Date.now() - cached.timestamp < this.cacheTimeout) {\n      console.log('Cache hit');\n      return { ...cached.response, fromCache: true };\n    }\n\n    // Generate new response\n    const response = await this.openai.chat.completions.create({\n      messages,\n      ...options\n    });\n\n    // Cache response\n    this.cache.set(cacheKey, {\n      response,\n      timestamp: Date.now()\n    });\n\n    // Manage cache size\n    if (this.cache.size > this.maxCacheSize) {\n      const oldestKey = this.cache.keys().next().value;\n      this.cache.delete(oldestKey);\n    }\n\n    return { ...response, fromCache: false };\n  }\n\n  clearCache() {\n    this.cache.clear();\n  }\n\n  getCacheStats() {\n    return {\n      size: this.cache.size,\n      maxSize: this.maxCacheSize,\n      utilization: this.cache.size / this.maxCacheSize\n    };\n  }\n}\n"})}),"\n",(0,o.jsx)(n.h3,{id:"batch-processing",children:"Batch Processing"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-javascript",children:"class BatchChatProcessor {\n  constructor(apiKey, options = {}) {\n    this.openai = new OpenAI({\n      apiKey: apiKey,\n      baseURL: 'https://api.conduit.yourdomain.com/v1'\n    });\n    \n    this.concurrency = options.concurrency || 5;\n    this.delay = options.delay || 100; // ms between requests\n  }\n\n  async processBatch(requests) {\n    const results = [];\n    \n    // Process in chunks to respect rate limits\n    for (let i = 0; i < requests.length; i += this.concurrency) {\n      const chunk = requests.slice(i, i + this.concurrency);\n      \n      const chunkPromises = chunk.map(async (request, index) => {\n        try {\n          // Add delay to stagger requests\n          await new Promise(resolve => setTimeout(resolve, index * this.delay));\n          \n          const response = await this.openai.chat.completions.create(request);\n          \n          return {\n            index: i + index,\n            success: true,\n            response: response,\n            request: request\n          };\n        } catch (error) {\n          return {\n            index: i + index,\n            success: false,\n            error: error.message,\n            request: request\n          };\n        }\n      });\n\n      const chunkResults = await Promise.allSettled(chunkPromises);\n      results.push(...chunkResults.map(result => result.value));\n      \n      console.log(`Processed batch ${Math.floor(i / this.concurrency) + 1}/${Math.ceil(requests.length / this.concurrency)}`);\n    }\n\n    return results;\n  }\n\n  generateBatchSummary(results) {\n    const successful = results.filter(r => r.success);\n    const failed = results.filter(r => !r.success);\n    \n    const totalTokens = successful.reduce((sum, r) => {\n      return sum + (r.response.usage?.total_tokens || 0);\n    }, 0);\n    \n    const totalCost = successful.reduce((sum, r) => {\n      return sum + (r.response.conduit_metadata?.cost || 0);\n    }, 0);\n\n    return {\n      totalRequests: results.length,\n      successful: successful.length,\n      failed: failed.length,\n      totalTokens: totalTokens,\n      totalCost: totalCost,\n      failureRate: failed.length / results.length,\n      errors: failed.map(f => ({ index: f.index, error: f.error }))\n    };\n  }\n}\n\n// Usage\nconst batchProcessor = new BatchChatProcessor('condt_your_virtual_key', {\n  concurrency: 3,\n  delay: 200\n});\n\nconst requests = [\n  {\n    model: 'gpt-3.5-turbo',\n    messages: [{ role: 'user', content: 'Summarize the benefits of renewable energy' }],\n    max_tokens: 200\n  },\n  {\n    model: 'gpt-3.5-turbo',\n    messages: [{ role: 'user', content: 'Explain quantum computing simply' }],\n    max_tokens: 200\n  },\n  {\n    model: 'gpt-3.5-turbo',\n    messages: [{ role: 'user', content: 'What are the advantages of electric vehicles?' }],\n    max_tokens: 200\n  }\n];\n\nconst results = await batchProcessor.processBatch(requests);\nconst summary = batchProcessor.generateBatchSummary(results);\n\nconsole.log('Batch processing summary:', summary);\n"})}),"\n",(0,o.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Function Calling"}),": Master ",(0,o.jsx)(n.a,{href:"../clients/nodejs-client#function-calling",children:"advanced function calling patterns"})]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Streaming"}),": Implement ",(0,o.jsx)(n.a,{href:"../realtime/overview",children:"real-time streaming responses"})]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Multimodal"}),": Explore ",(0,o.jsx)(n.a,{href:"../media/overview",children:"image and video capabilities"})]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Audio Integration"}),": Combine with ",(0,o.jsx)(n.a,{href:"../audio/overview",children:"speech services"})]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>r});var s=t(6540);const o={},a=s.createContext(o);function i(e){const n=s.useContext(a);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);