"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[117],{3672:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>a,contentTitle:()=>l,default:()=>c,frontMatter:()=>r,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"features/multimodal-support","title":"Multimodal Support","description":"Learn how to use Conduit for multimodal AI capabilities including vision and image generation","source":"@site/docs/features/multimodal-support.md","sourceDirName":"features","slug":"/features/multimodal-support","permalink":"/Conduit/docs/features/multimodal-support","draft":false,"unlisted":false,"editUrl":"https://github.com/knnlabs/Conduit/tree/main/website/docs/features/multimodal-support.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5,"title":"Multimodal Support","description":"Learn how to use Conduit for multimodal AI capabilities including vision and image generation"},"sidebar":"docsSidebar","previous":{"title":"Provider Integration","permalink":"/Conduit/docs/features/provider-integration"},"next":{"title":"Audio Services","permalink":"/Conduit/docs/features/audio-services"}}');var s=n(4848),o=n(8453);const r={sidebar_position:5,title:"Multimodal Support",description:"Learn how to use Conduit for multimodal AI capabilities including vision and image generation"},l="Multimodal Support",a={},d=[{value:"Vision Models",id:"vision-models",level:2},{value:"Using Vision Models",id:"using-vision-models",level:3},{value:"Supported Image Formats",id:"supported-image-formats",level:3},{value:"Image Input Methods",id:"image-input-methods",level:3},{value:"Image Generation",id:"image-generation",level:2},{value:"Image Generation Providers",id:"image-generation-providers",level:3},{value:"Audio Processing",id:"audio-processing",level:2},{value:"Working with Multiple Modalities",id:"working-with-multiple-modalities",level:2},{value:"Provider Capabilities",id:"provider-capabilities",level:2},{value:"Next Steps",id:"next-steps",level:2}];function u(e){const i={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(i.header,{children:(0,s.jsx)(i.h1,{id:"multimodal-support",children:"Multimodal Support"})}),"\n",(0,s.jsx)(i.p,{children:"Conduit provides support for multimodal AI capabilities, allowing you to work with images, text, and other data types through a unified API."}),"\n",(0,s.jsx)(i.h2,{id:"vision-models",children:"Vision Models"}),"\n",(0,s.jsx)(i.p,{children:"Conduit supports vision-enabled models from various providers, enabling you to analyze images and process them alongside text."}),"\n",(0,s.jsx)(i.h3,{id:"using-vision-models",children:"Using Vision Models"}),"\n",(0,s.jsx)(i.p,{children:"To use a vision model with Conduit, send a chat completion request with image content:"}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-json",children:'{\n  "model": "my-gpt4-vision",\n  "messages": [\n    {\n      "role": "user",\n      "content": [\n        {\n          "type": "text",\n          "text": "What\'s in this image?"\n        },\n        {\n          "type": "image_url",\n          "image_url": {\n            "url": "data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAA..."\n          }\n        }\n      ]\n    }\n  ]\n}\n'})}),"\n",(0,s.jsx)(i.h3,{id:"supported-image-formats",children:"Supported Image Formats"}),"\n",(0,s.jsx)(i.p,{children:"Conduit supports multiple image formats:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"JPEG"}),"\n",(0,s.jsx)(i.li,{children:"PNG"}),"\n",(0,s.jsx)(i.li,{children:"WebP"}),"\n",(0,s.jsx)(i.li,{children:"GIF (first frame only for some providers)"}),"\n"]}),"\n",(0,s.jsx)(i.h3,{id:"image-input-methods",children:"Image Input Methods"}),"\n",(0,s.jsx)(i.p,{children:"You can provide images in several ways:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Base64-encoded data URLs"}),"\n",(0,s.jsx)(i.li,{children:"HTTP/HTTPS URLs to publicly accessible images"}),"\n",(0,s.jsx)(i.li,{children:"Local file paths (for self-hosted deployments only)"}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"image-generation",children:"Image Generation"}),"\n",(0,s.jsx)(i.p,{children:"Conduit also supports image generation through compatible providers:"}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-json",children:'{\n  "prompt": "A serene mountain landscape at sunset",\n  "model": "my-dall-e",\n  "n": 1,\n  "size": "1024x1024"\n}\n'})}),"\n",(0,s.jsx)(i.h3,{id:"image-generation-providers",children:"Image Generation Providers"}),"\n",(0,s.jsx)(i.p,{children:"Conduit supports image generation through:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"OpenAI (DALL-E)"}),"\n",(0,s.jsx)(i.li,{children:"Stability AI (if configured)"}),"\n",(0,s.jsx)(i.li,{children:"Midjourney (through integration)"}),"\n",(0,s.jsx)(i.li,{children:"Other compatible providers"}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"audio-processing",children:"Audio Processing"}),"\n",(0,s.jsx)(i.p,{children:"Some providers offer audio processing capabilities, which Conduit can expose:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Speech-to-text transcription"}),"\n",(0,s.jsx)(i.li,{children:"Text-to-speech synthesis"}),"\n",(0,s.jsx)(i.li,{children:"Audio analysis"}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"These features are available through dedicated endpoints with the same authentication and routing mechanisms as text-based models."}),"\n",(0,s.jsx)(i.h2,{id:"working-with-multiple-modalities",children:"Working with Multiple Modalities"}),"\n",(0,s.jsx)(i.p,{children:"Conduit provides a standardized way to combine different modalities in your requests:"}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-json",children:'{\n  "model": "my-multimodal-model",\n  "messages": [\n    {\n      "role": "user",\n      "content": [\n        {\n          "type": "text",\n          "text": "Summarize the contents of this image and audio clip"\n        },\n        {\n          "type": "image_url",\n          "image_url": {\n            "url": "https://example.com/image.jpg"\n          }\n        },\n        {\n          "type": "audio_url",\n          "audio_url": {\n            "url": "https://example.com/audio.mp3"\n          }\n        }\n      ]\n    }\n  ]\n}\n'})}),"\n",(0,s.jsx)(i.h2,{id:"provider-capabilities",children:"Provider Capabilities"}),"\n",(0,s.jsx)(i.p,{children:"Not all providers support all modalities. Conduit's provider capabilities detection helps identify which models can handle different input types."}),"\n",(0,s.jsx)(i.p,{children:"To check model capabilities:"}),"\n",(0,s.jsxs)(i.ol,{children:["\n",(0,s.jsxs)(i.li,{children:["Navigate to ",(0,s.jsx)(i.strong,{children:"Models"})," in the Web UI"]}),"\n",(0,s.jsx)(i.li,{children:"View the capabilities column for each model"}),"\n",(0,s.jsx)(i.li,{children:"Filter models by capability"}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["Explore ",(0,s.jsx)(i.a,{href:"model-routing",children:"Model Routing"})," to understand how requests are directed to providers"]}),"\n",(0,s.jsxs)(i.li,{children:["Learn about ",(0,s.jsx)(i.a,{href:"provider-integration",children:"Provider Integration"})," for adding new multimodal services"]}),"\n",(0,s.jsxs)(i.li,{children:["See the ",(0,s.jsx)(i.a,{href:"../api-reference/overview",children:"API Reference"})," for detailed endpoint documentation"]}),"\n"]})]})}function c(e={}){const{wrapper:i}={...(0,o.R)(),...e.components};return i?(0,s.jsx)(i,{...e,children:(0,s.jsx)(u,{...e})}):u(e)}},8453:(e,i,n)=>{n.d(i,{R:()=>r,x:()=>l});var t=n(6540);const s={},o=t.createContext(s);function r(e){const i=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(i):{...i,...e}}),[i,e])}function l(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(o.Provider,{value:i},e.children)}}}]);