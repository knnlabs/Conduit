# Conduit Provider Compatibility Report

**Last Updated**: 2025-08-14  
**Status**: Canonical Reference Document

## Executive Summary

This document provides a comprehensive analysis of Conduit's compatibility with all supported LLM providers, excluding OpenAI. It serves as the canonical source of truth for understanding provider capabilities, implementation details, and compatibility gaps.

## Provider Classification

### 1. OpenAI-Compatible Providers
Providers that inherit from `OpenAICompatibleClient` and use OpenAI's API format:
- **Groq** - High-speed inference provider
- **Cerebras** - Ultra-fast inference with custom hardware
- **SambaNova** - Enterprise-grade fast inference
- **Fireworks** - Multi-model inference platform
- **DeepInfra** - Cost-effective inference platform

### 2. Custom Implementation Providers
Providers with unique APIs requiring custom implementations:
- **Replicate** (`CustomProviderClient`) - Prediction-based API with polling
- **MiniMax** (`BaseLLMClient`) - Chinese provider with unique features

### 3. Specialized Providers
- **Ultravox** - Audio/voice processing (limited scope)
- **ElevenLabs** - Text-to-speech only (not an LLM)

## Feature Support Matrix

| Provider | Chat | Stream | Embeddings | Images | Video | Audio | Tools | Vision | Overall |
|----------|------|--------|------------|--------|-------|-------|-------|--------|---------|
| **Groq** | ‚úÖ 100% | ‚úÖ 100% | ‚úÖ 100% | ‚ùå | ‚ùå | ‚ùå | ‚úÖ 100% | ‚ö†Ô∏è 50% | **90%** |
| **Cerebras** | ‚úÖ 100% | ‚úÖ 100% | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚ùå | **75%** |
| **SambaNova** | ‚úÖ 100% | ‚úÖ 100% | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚úÖ 100% | ‚ö†Ô∏è 50% | **85%** |
| **Fireworks** | ‚úÖ 100% | ‚úÖ 100% | ‚úÖ 100% | ‚úÖ 90% | ‚ùå | ‚ùå | ‚úÖ 100% | ‚úÖ 100% | **95%** |
| **DeepInfra** | ‚úÖ 100% | ‚úÖ 100% | ‚úÖ 100% | ‚úÖ 90% | ‚ùå | ‚ùå | ‚úÖ 100% | ‚úÖ 100% | **95%** |
| **Replicate** | ‚úÖ 80% | ‚úÖ 80% | ‚ö†Ô∏è 70% | ‚úÖ 95% | ‚úÖ 95% | ‚ùå | ‚ùå | ‚úÖ 90% | **70%** |
| **MiniMax** | ‚úÖ 85% | ‚úÖ 85% | ‚ùå | ‚úÖ 90% | ‚úÖ 95% | ‚ùå | ‚ùå | ‚úÖ 80% | **75%** |

### Legend
- ‚úÖ Full support with high compatibility
- ‚ö†Ô∏è Partial support or requires specific models
- ‚ùå Not supported

## Detailed Provider Analysis

### üöÄ Groq
**API Base**: `https://api.groq.com/openai/v1`  
**Implementation**: `GroqClient : OpenAICompatibleClient`

#### Strengths
- **Perfect OpenAI Compatibility**: Drop-in replacement for OpenAI chat endpoints
- **Industry-Leading Speed**: Uses custom LPU (Language Processing Unit) hardware
- **Full Streaming Support**: SSE streaming works identically to OpenAI
- **Tool/Function Calling**: Complete support for OpenAI tool calling format
- **Embeddings**: Native support for embedding models

#### Limitations
- **No Image Generation**: Text-only provider
- **Limited Vision Support**: Only specific models (Llama 3.2 Vision) support images
- **No Audio Features**: No TTS or transcription

#### Supported Models
- Llama 3.1 (8B, 70B, 405B)
- Llama 3.2 Vision (11B, 90B)
- Mixtral 8x7B
- Gemma 2 (9B, 27B)

#### Parameter Compatibility
```json
{
  "temperature": "0-2 (OpenAI compatible)",
  "max_tokens": "Fully supported",
  "top_p": "0-1 supported",
  "stream": "Perfect SSE implementation",
  "tools": "Full OpenAI format",
  "response_format": "JSON mode supported"
}
```

---

### ‚ö° Cerebras
**API Base**: `https://api.cerebras.ai/v1`  
**Implementation**: `CerebrasClient : OpenAICompatibleClient`

#### Strengths
- **Fastest Inference**: 2,000+ tokens/second on Llama models
- **Simple Integration**: Minimal parameters, clean API
- **Excellent Streaming**: Low-latency SSE streaming
- **High Reliability**: Enterprise-grade infrastructure

#### Limitations
- **Text-Only**: No embeddings, images, or multimodal support
- **No Tool Calling**: Pure text generation only
- **Limited Models**: Focus on Llama family

#### Supported Models
- Llama 3.1 (8B, 70B)
- Llama 3.3 (70B)

#### Parameter Compatibility
```json
{
  "temperature": "0-1.5 range",
  "max_tokens": "Supported with high limits",
  "top_p": "Standard 0-1",
  "stream": "Optimized for speed"
}
```

---

### üèÉ SambaNova
**API Base**: `https://api.sambanova.ai/v1`  
**Implementation**: `SambaNovaClient : OpenAICompatibleClient`

#### Strengths
- **Enterprise Focus**: Built for production workloads
- **OpenAI Compatible**: Uses OpenAI client libraries directly
- **Tool Support**: Full function calling capabilities
- **Fast Inference**: Competitive speeds with custom chips

#### Limitations
- **No Embeddings Endpoint**: Text generation only
- **No Image Generation**: Not supported
- **Documentation**: Less comprehensive than competitors

#### Supported Models
- Llama 3.1 (8B, 70B, 405B)
- Llama 3.2 (1B, 3B)
- Custom fine-tuned models

#### Parameter Compatibility
```json
{
  "temperature": "OpenAI compatible",
  "max_tokens": "Fully supported",
  "tools": "OpenAI format",
  "stream": "Standard SSE"
}
```

---

### üî• Fireworks
**API Base**: `https://api.fireworks.ai/inference/v1`  
**Implementation**: `FireworksClient : OpenAICompatibleClient`

#### Strengths
- **Most Complete Compatibility**: Nearest to full OpenAI parity
- **Multi-Feature Support**: Chat, embeddings, images all work
- **Extensive Model Library**: 100+ models available
- **Vision Support**: Multiple vision-capable models
- **Developer Friendly**: Excellent documentation

#### Limitations
- **No Video Generation**: Images only
- **Minor Parameter Differences**: Some image params differ from OpenAI

#### Supported Models
- Llama family (all versions)
- Mixtral models
- Stable Diffusion (images)
- Custom fine-tuned models
- Vision models (Llava, etc.)

#### Parameter Compatibility
```json
{
  "temperature": "Full OpenAI range",
  "max_tokens": "Model-specific limits",
  "tools": "Complete support",
  "response_format": "JSON mode works",
  "images": "DALL-E compatible params"
}
```

---

### üåä DeepInfra
**API Base**: `https://api.deepinfra.com/v1/openai`  
**Implementation**: `DeepInfraClient : OpenAICompatibleClient`

#### Strengths
- **Cost Effective**: Lower pricing than most competitors
- **Wide Model Selection**: 200+ models available
- **Full OpenAI Compatibility**: Drop-in replacement
- **Image Generation**: Stable Diffusion and other models
- **Vision Support**: Multiple multimodal models

#### Limitations
- **No Video**: Images only for media generation
- **Variable Performance**: Speed depends on model/load

#### Supported Models
- All major open-source LLMs
- Stable Diffusion XL, SD3
- Whisper (transcription)
- Vision models

#### Parameter Compatibility
```json
{
  "temperature": "OpenAI compatible",
  "max_tokens": "Supported",
  "tools": "Full support",
  "response_format": "JSON mode",
  "embeddings": "Multiple models"
}
```

---

### üé® Replicate
**API Base**: `https://api.replicate.com/v1/`  
**Implementation**: `ReplicateClient : CustomProviderClient`

#### Unique Architecture
- **Prediction-Based API**: Async predictions with polling
- **Model Versioning**: Uses version hashes for reproducibility
- **Flexible Input**: Accepts varied parameter formats

#### Strengths
- **Best Video Support**: Wide range of video generation models
- **Unique Models**: Access to cutting-edge research models
- **Image Excellence**: Top-tier image generation
- **Custom Models**: Easy deployment of custom models

#### Limitations
- **Different Paradigm**: Not OpenAI-compatible
- **Polling Required**: Async API requires status checking
- **Parameter Mapping**: Custom parameter names per model
- **No Native Streaming**: Simulated streaming only

#### Supported Models
- Stable Diffusion variants
- FLUX image models
- Video generation (multiple)
- Custom deployed models
- LLMs (Llama, Mistral, etc.)

#### API Differences
```python
# Replicate flow
prediction = start_prediction(model, input)
while prediction.status != "succeeded":
    prediction = get_prediction(prediction.id)
    sleep(2)
result = prediction.output
```

---

### üá®üá≥ MiniMax
**API Base**: `https://api.minimax.io`  
**Implementation**: `MiniMaxClient : BaseLLMClient`

#### Strengths
- **Native Video Generation**: Built-in video support
- **Good Image Generation**: Quality image models
- **Solid Chat**: Reliable text generation
- **Unique Features**: China-specific capabilities

#### Limitations
- **Language Barrier**: Chinese documentation
- **No Embeddings**: Text/image/video only
- **Different Parameters**: Non-OpenAI naming
- **No Tool Calling**: Basic chat only
- **Regional Focus**: Optimized for Chinese market

#### Supported Features
- Chat completions (custom format)
- Image generation (custom params)
- Video generation (unique feature)
- Streaming (custom implementation)

#### Parameter Mapping
```json
{
  "prompt": "Maps to 'messages'",
  "max_length": "Instead of max_tokens",
  "sampling_temperature": "Instead of temperature",
  "top_p_ratio": "Instead of top_p"
}
```

## Critical Compatibility Issues

### 1. Parameter Normalization Challenges

#### Temperature Ranges
- OpenAI: 0-2
- Groq: 0-2
- Cerebras: 0-1.5
- Others: Varies

**Current Handling**: Passed through without conversion (may cause errors)

#### Token Limits
- `max_tokens` (OpenAI standard)
- `max_new_tokens` (Hugging Face style)
- `max_length` (MiniMax)

**Current Handling**: Mapped in provider-specific clients

### 2. Response Format Variations

#### Streaming Formats
- **OpenAI-Compatible**: Standard SSE with `data: ` prefix
- **Replicate**: No native streaming, polling-based
- **MiniMax**: Custom streaming format

#### Error Responses
Each provider returns errors differently:
- Groq/Cerebras/etc: OpenAI error format
- Replicate: Custom error in prediction object
- MiniMax: Localized error messages

### 3. Feature Detection Gaps

**Problem**: No runtime capability detection
**Impact**: Errors when unsupported features are requested
**Example**: Requesting embeddings from Cerebras fails with unclear error

## Recommendations for Improvement

### 1. Implement Provider Capability System
```csharp
public interface IProviderCapabilities
{
    bool SupportsChat { get; }
    bool SupportsStreaming { get; }
    bool SupportsEmbeddings { get; }
    bool SupportsImages { get; }
    bool SupportsVideo { get; }
    bool SupportsTools { get; }
    bool SupportsVision { get; }
    
    ParameterRanges GetParameterRanges();
}
```

### 2. Add Parameter Adapters
```csharp
public class ParameterAdapter
{
    public double NormalizeTemperature(double input, Provider provider)
    {
        return provider.Type switch
        {
            ProviderType.Cerebras => Math.Min(input * 0.75, 1.5),
            _ => input
        };
    }
}
```

### 3. Unified Error Mapping
```csharp
public class ErrorMapper
{
    public ConduitError MapProviderError(object error, ProviderType type)
    {
        // Standardize all provider errors to common format
    }
}
```

### 4. Feature Fallbacks
- **Embeddings**: Route to Fireworks/DeepInfra when provider doesn't support
- **Images**: Route to Replicate when provider is text-only
- **Video**: Only Replicate/MiniMax - need clear messaging

## Missing Critical Features by Provider

### High Priority Gaps
1. **Audio Support**: Almost no providers support TTS/transcription
   - Only OpenAI, ElevenLabs, Ultravox
   - Consider adding Whisper via DeepInfra

2. **Video Generation**: Only 2 providers
   - Replicate (best support)
   - MiniMax (limited models)
   - High user demand, limited options

3. **Embeddings**: Missing from major providers
   - Cerebras ‚ùå
   - SambaNova ‚ùå
   - MiniMax ‚ùå

### Provider-Specific Improvements Needed

#### Groq
- Add image generation via partnership/proxy
- Expand vision model support

#### Cerebras
- Add embeddings endpoint
- Consider tool calling support

#### SambaNova
- Implement embeddings
- Add image generation

#### Replicate
- Improve streaming simulation
- Add proper embeddings support
- Optimize polling mechanism

#### MiniMax
- English documentation needed
- Add embeddings support
- Implement tool calling

## Compatibility Scores

### Overall Rankings
1. **Fireworks** - 95% (most complete)
2. **DeepInfra** - 95% (excellent compatibility)
3. **Groq** - 90% (perfect for text+tools)
4. **SambaNova** - 85% (enterprise ready)
5. **Cerebras** - 75% (fast but limited)
6. **MiniMax** - 75% (unique features, different paradigm)
7. **Replicate** - 70% (powerful but different)

### By Use Case

#### Best for Chat/Text
1. Groq (speed + compatibility)
2. Cerebras (pure speed)
3. Fireworks (features)

#### Best for Multimodal
1. Fireworks
2. DeepInfra
3. Replicate

#### Best for Media Generation
1. Replicate (images + video)
2. MiniMax (video specialty)
3. Fireworks (images only)

#### Best for Embeddings
1. Groq
2. Fireworks
3. DeepInfra

## Implementation Status

### Well Implemented ‚úÖ
- OpenAI-compatible chat for all compatible providers
- Streaming for all providers (including simulated)
- Error handling and retry logic
- Authentication verification

### Needs Improvement ‚ö†Ô∏è
- Parameter normalization
- Capability detection
- Provider-specific optimizations
- Feature availability messaging

### Missing ‚ùå
- Runtime capability queries
- Automatic fallback routing
- Provider health monitoring
- Cost optimization routing

## Maintenance Notes

### Provider API Versions
- Groq: OpenAI v1 compatible
- Cerebras: v1 API
- SambaNova: v1 API
- Fireworks: v1 inference API
- DeepInfra: v1/openai endpoint
- Replicate: v1 predictions
- MiniMax: Proprietary versioning

### Update Frequency
- This document should be reviewed monthly
- Provider capabilities change frequently
- New models may add/remove features

### Testing Requirements
Each provider should be tested for:
1. Basic chat completion
2. Streaming responses
3. Error handling
4. Feature availability
5. Parameter boundaries

## Conclusion

Conduit achieves excellent compatibility with OpenAI-compatible providers (85-95%) while supporting unique providers like Replicate and MiniMax with custom implementations. The main gaps are in standardizing parameter handling, implementing capability detection, and providing clear feature availability messaging to users.

The OpenAI-compatible providers (Groq, Fireworks, DeepInfra) offer near-perfect compatibility and should be preferred for users migrating from OpenAI. Replicate and MiniMax provide unique capabilities (especially video) that aren't available elsewhere, justifying their custom implementations despite lower compatibility scores.

---

*This document represents the state of provider compatibility as of 2025-08-14 and should be updated as providers evolve and new features are added to Conduit.*